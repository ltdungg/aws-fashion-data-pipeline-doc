var relearn_searchindex = [
  {
    "breadcrumb": "AWS Fashion Pipeline",
    "content": "A Data Pipeline for Fashion Store Dữ liệu Workshop này bao gồm những dữ liệu sau:\nCác sản phẩm quần áo được lấy từ Kaggle Dữ liệu người dùng, mua hàng, clickstream tự tạo bằng Faker Python Bối cảnh Giả sử bạn là Data Engineer tại một cửa hàng bán quần áo online, sếp bạn muốn biết sự quan tâm của khách hàng vào những sản phẩm nào cũng như là phân tích xem xu hướng thời trang hiện tại như thế nào.\nSếp của bạn có những yêu cầu sau:\nKhi người dùng bấm vào hoặc thêm vào giỏ hàng một sản phẩm, trực tiếp được chuyển vào kho lưu trữ, đồng thời chuyển đổi và làm giàu để ML có thể dự đoán xu hướng của thời trang hiện tại và đưa ra các gợi ý sản phẩm cho người dùng real-time. Sau đó dữ liệu gợi ý cho từng User được lưu vào nơi để có thể trực tiếp gợi ý cho người dùng ở trên webiste. Yêu cầu thứ 2 là, sau khi người dùng mua hàng, thì kết thúc một ngày, dữ liệu cần được tổng hợp, làm sạch và chuyển vào kho lưu trữ để sẵn sàng cho mục đích phân tích để tìm ra hướng cho chiến lược marketing tiếp theo. Và ngay sau đó bạn nảy ra ý tưởng triển khai trên Amazon Web Service như sau.\nÝ tưởng Bạn nghĩ ra một hệ thống kết hợp giữa Batch Processing và Real-Time Processing như sau:\nTriển khai hệ thống Database trên Amazon RDS, nơi xử lý các giao dịch mua hàng của người dùng,…\nTriển khai hệ thống Data Lake bằng Amazon S3 trên AWS để lưu trữ dữ liệu tổng hợp của cửa hàng.\nXây dựng Glue Data Catalog để có thể trực tiếp phân tích dữ liệu từ Data Lake bằng Athena, hoặc Redshift Spectrum\nĐối với luồng dữ liệu tương tác khách hàng (Clickstream), ta sẽ tạo một Kinesis Data Stream sau đó có 2 Consumer:\nDữ liệu trực tuyến đi qua Lambda, và mỗi 5 phút sẽ tổng hợp dữ liệu Clickstream của người dùng đó rồi đưa ra gợi ý sản phẩm đẩy vào DynamoDB. Consumer còn lại đi vào Kinesis Firehose, để đẩy vào Data Lake, nơi Data Analysis phân tích dữ liệu và đưa ra xu hướng thời trang, cũng như là doanh số,… Lên lịch cho những công việc với Amazon MWAA\nDữ liệu clickstream đi vào Lambda mỗi 5 phút. Sau một ngày dữ liệu từ RDS đi vào S3 vào 00:01 ngày hôm sau. Sau lúc bạn ghi ra những ý tưởng, đầu bạn nảy ra một hệ thống trên AWS có kiến trúc như sau\nKiến trúc hệ thống",
    "description": "A Data Pipeline for Fashion Store Dữ liệu Workshop này bao gồm những dữ liệu sau:\nCác sản phẩm quần áo được lấy từ Kaggle Dữ liệu người dùng, mua hàng, clickstream tự tạo bằng Faker Python Bối cảnh Giả sử bạn là Data Engineer tại một cửa hàng bán quần áo online, sếp bạn muốn biết sự quan tâm của khách hàng vào những sản phẩm nào cũng như là phân tích xem xu hướng thời trang hiện tại như thế nào.",
    "tags": [],
    "title": "1. Giới thiệu",
    "uri": "/vi/overview/index.html"
  },
  {
    "breadcrumb": "AWS Fashion Pipeline \u003e  2. Các bước chuẩn bị",
    "content": "Tạo role cho Lambda truy cập vào S3 Truy cập vào AWS Console, chọn IAM từ menu dịch vụ. Chọn Roles từ menu bên trái, bấm vào Create role. Tại giao diện Select trusted entity, chọn AWS service, sau đó chọn Lambda từ danh sách dịch vụ. Bấm Next. Tại phần Add permissions\nTìm kiếm và chọn AmazonS3FullAccess từ danh sách chính sách. Tìm kiếm và chọn AWSLambdaENIManagementAccess từ danh sách chính sách. Bấm Next. Tại phần Name, review, and create\nNhập tên cho role là lambda-s3-full-access. Phần description nhập Allow Lambda access S3 with Full Access. Bấm Create role. Tạo role cho EC2 truy cập vào Kinesis Truy cập vào AWS Console, chọn IAM từ menu dịch vụ. Chọn Roles từ menu bên trái, bấm vào Create role. Tại giao diện Select trusted entity, chọn AWS service, sau đó chọn EC2 từ danh sách dịch vụ. Bấm Next. Tại phần Add permissions Tìm kiếm và chọn AmazonKinesisFullAccess từ danh sách chính sách. Bấm Next. Tại phần Name, review, and create Nhập tên cho role là ec2-kinesis-role. Phần description nhập Allow EC2 access Kinesis with Full Access. Bấm Create role.",
    "description": "Tạo role cho Lambda truy cập vào S3 Truy cập vào AWS Console, chọn IAM từ menu dịch vụ. Chọn Roles từ menu bên trái, bấm vào Create role. Tại giao diện Select trusted entity, chọn AWS service, sau đó chọn Lambda từ danh sách dịch vụ. Bấm Next. Tại phần Add permissions\nTìm kiếm và chọn AmazonS3FullAccess từ danh sách chính sách. Tìm kiếm và chọn AWSLambdaENIManagementAccess từ danh sách chính sách. Bấm Next. Tại phần Name, review, and create",
    "tags": [],
    "title": "2.1 Tạo các Role cần thiết",
    "uri": "/vi/preparation/setup-role/index.html"
  },
  {
    "breadcrumb": "AWS Fashion Pipeline \u003e  5. Test dữ liệu",
    "content": "Kiểm tra dữ liệu trong RDS. Sử dụng lệnh truy cập vào PostgreSQL ở bước trước. psql -U postgres -h \u003cYOUR_POSTGRESQL_DNS\u003e -p 5432 -d postgres Nếu bạn thấy thông báo như sau thì bạn đã kết nối thành công tới database PostgreSQL. Chạy lệnh sau để kiểm tra dữ liệu trong bảng orders:\nSELECT * FROM orders; Chạy lệnh sau để kiểm tra dữ liệu trong bảng order_details: SELECT * FROM order_details; Dưới đây là hình ảnh bảng orders ví dụ, có thể dữ liệu sẽ khác nhau do random. Dưới đây là hình ảnh bảng order_details ví dụ, có thể dữ liệu sẽ khác nhau do random. Gõ \\q để thoát khỏi database PostgreSQL. và trở về terminal. Vậy là bạn đã hoàn tất việc kiểm tra dữ liệu trong RDS. Bạn có thể sử dụng các câu lệnh SQL khác để kiểm tra dữ liệu trong các bảng khác như users, products hoặc thực hiện các truy vấn phức tạp hơn.",
    "description": "Kiểm tra dữ liệu trong RDS. Sử dụng lệnh truy cập vào PostgreSQL ở bước trước. psql -U postgres -h \u003cYOUR_POSTGRESQL_DNS\u003e -p 5432 -d postgres Nếu bạn thấy thông báo như sau thì bạn đã kết nối thành công tới database PostgreSQL. Chạy lệnh sau để kiểm tra dữ liệu trong bảng orders:\nSELECT * FROM orders; Chạy lệnh sau để kiểm tra dữ liệu trong bảng order_details: SELECT * FROM order_details; Dưới đây là hình ảnh bảng orders ví dụ, có thể dữ liệu sẽ khác nhau do random.",
    "tags": [],
    "title": "5.1 Kiểm tra dữ liệu trong RDS",
    "uri": "/vi/test-connection/test-rds/index.html"
  },
  {
    "breadcrumb": "AWS Fashion Pipeline",
    "content": "Tổng quan Đầu tiên là tạo một môi trường ảo, coi như đây là môi trường ứng dụng của cửa hàng. EC2 Instance sẽ là máy chủ giả, nơi tạo ra dữ liệu cho lab. RDS Instance sẽ là cơ sở dữ liệu, dành cho các giao dịch… S3 Bucket sẽ là Data Lake, bao gồm các bucket sau: fashion-landing-zone: Nơi lưu trữ dữ liệu thô từ các nguồn khác nhau. fashion-clean-zone: Nơi lưu trữ dữ liệu đã được xử lý và chuẩn bị cho việc phân tích. fashion-logic-zone: Nơi lưu trữ các script Lambda và mã nguồn cho các quy trình ETL (Extract, Transform, Load). Kinesis Stream sẽ là luồng lưu trữ các sự kiện tương tác của khách hàng Các bước triển khai Đảm bảo để Region là Singapore (ap-southeast-1) trước khi thực hiện các bước sau:\n2.1 Tạo các Role cần thiết\n2.2 Tạo VPC, Subnet, …\n2.3 Tạo EC2 Instance\n2.4 Tạo RDS Instance\n2.5 Tạo S3 Bucket\n2.6 Tạo Kinesis Stream",
    "description": "Tổng quan Đầu tiên là tạo một môi trường ảo, coi như đây là môi trường ứng dụng của cửa hàng. EC2 Instance sẽ là máy chủ giả, nơi tạo ra dữ liệu cho lab. RDS Instance sẽ là cơ sở dữ liệu, dành cho các giao dịch… S3 Bucket sẽ là Data Lake, bao gồm các bucket sau: fashion-landing-zone: Nơi lưu trữ dữ liệu thô từ các nguồn khác nhau. fashion-clean-zone: Nơi lưu trữ dữ liệu đã được xử lý và chuẩn bị cho việc phân tích. fashion-logic-zone: Nơi lưu trữ các script Lambda và mã nguồn cho các quy trình ETL (Extract, Transform, Load). Kinesis Stream sẽ là luồng lưu trữ các sự kiện tương tác của khách hàng Các bước triển khai Đảm bảo để Region là Singapore (ap-southeast-1) trước khi thực hiện các bước sau:",
    "tags": [],
    "title": "2. Các bước chuẩn bị",
    "uri": "/vi/preparation/index.html"
  },
  {
    "breadcrumb": "AWS Fashion Pipeline \u003e  2. Các bước chuẩn bị",
    "content": "Tạo VPC (Virtual Private Cloud) Truy cập Amazon Management Console Tìm kiếm dịch vụ VPC Chọn VPC từ kết quả tìm kiếm Trong phần giao diện VPC chọn Create VPC Trong giao diện Create VPC Chọn VPC and more name tag auto-generation ghi: fashion IPv4 CIDR block: 10.10.0.0/16 Còn lại để mặc định, chọn Create VPC ở phía bên dưới màn hình.",
    "description": "Tạo VPC (Virtual Private Cloud) Truy cập Amazon Management Console Tìm kiếm dịch vụ VPC Chọn VPC từ kết quả tìm kiếm Trong phần giao diện VPC chọn Create VPC Trong giao diện Create VPC Chọn VPC and more name tag auto-generation ghi: fashion IPv4 CIDR block: 10.10.0.0/16 Còn lại để mặc định, chọn Create VPC ở phía bên dưới màn hình.",
    "tags": [],
    "title": "2.2 Tạo VPC, Subnet, ...",
    "uri": "/vi/preparation/setup-env/index.html"
  },
  {
    "breadcrumb": "AWS Fashion Pipeline \u003e  5. Test dữ liệu",
    "content": "Kiểm tra dữ liệu trong Kinesis Data Stream Quay trở lại terminal kết nối với EC2 Instance tại bước 2.3 Tạo EC2 Instance. Chạy lệnh sau để mô tả luồng Kinesis Data Stream: aws kinesis describe-stream --stream-name fashion-ds Chạy lệnh sau để lấy iterator của shard trong luồng Kinesis Data Stream: aws kinesis get-shard-iterator --stream-name fashion-ds --shard-id shardId-000000000000 --shard-iterator-type TRIM_HORIZON Copy ShardIterator trong kết quả trả về của lệnh trên và chạy lệnh sau để lấy dữ liệu trong luồng Kinesis Data Stream: aws kinesis get-records --shard-iterator \u003cYOUR_SHARD_ITERATOR\u003e Trong phần Records:\nPartitionKey là user ID Data là những dữ liệu JSON được encode trong luồng Kiểm tra dữ liệu, copy một Data bên trong Records. Vào trang Base64 Decode để decode dữ liệu vừa copy.\nDecode UTF-8 Bạn phải decode dữ liệu 2 lần mới ra được dữ liệu JSON. Tại vì mình viết code encode 2 lần :)) Tức là copy kết quả decode đầu tiên và paste vào lại để decode lần 2.",
    "description": "Kiểm tra dữ liệu trong Kinesis Data Stream Quay trở lại terminal kết nối với EC2 Instance tại bước 2.3 Tạo EC2 Instance. Chạy lệnh sau để mô tả luồng Kinesis Data Stream: aws kinesis describe-stream --stream-name fashion-ds Chạy lệnh sau để lấy iterator của shard trong luồng Kinesis Data Stream: aws kinesis get-shard-iterator --stream-name fashion-ds --shard-id shardId-000000000000 --shard-iterator-type TRIM_HORIZON Copy ShardIterator trong kết quả trả về của lệnh trên và chạy lệnh sau để lấy dữ liệu trong luồng Kinesis Data Stream: aws kinesis get-records --shard-iterator \u003cYOUR_SHARD_ITERATOR\u003e",
    "tags": [],
    "title": "5.2 Kiểm tra dữ liệu trong Kinesis Data Stream",
    "uri": "/vi/test-connection/test-kinesis/index.html"
  },
  {
    "breadcrumb": "AWS Fashion Pipeline \u003e  2. Các bước chuẩn bị",
    "content": "Tạo Elastic Compute Cloud (EC2) Instance Truy cập Amazon Management Console Tìm kiếm dịch vụ EC2 Chọn EC2 từ kết quả tìm kiếm Chọn Launch Instance Trong giao diện Launch Instance Name: fashion-webapp Amazon Machine Image (AMI): Amazon Linux 2023 AMI Instance Type: t2.micro Tại phần Key pair, chọn Create new key pair\nKey pair name: fashion-keypair Key pair type: RSA, private key file format: .pem Chọn Create key pair Tại phần Network Settings, chọn Edit:\nVPC: fashion-vpc Subnet: fashion-subnet-public1-ap-southeast-1a Auto-assign Public IP: Enable Security Group chọn Create security group security group name: fashion-webapp-sg description: Allow SSH from My IP, and HTTP, HTTPS from everywhere Tại Inbound Security Group Rules, cài đặt như hình sau Còn lại để mặc định, nhấn Launch instance để hoàn tất việc tạo EC2 Instance Cài đặt môi trường bên trong EC2 Instance Kết nối SSH vào EC2 Instance Di chuyển tới thư mục chứa file fashion-keypair.pem Sử dụng terminal, gõ lệnh sau: ssh -i fashion-keypair.pem ec2-user@\u003cPublic IP | Public DNS\u003e Gõ ‘yes’ Nếu không kết nối được, hãy kiểm tra lại Security Group đã cho phép SSH từ IP của bạn chưa nhé. Cài đặt môi trường Python, Git, PostgreSQL cho EC2 Instance Đầu tiên, gõ sudo su để chuyển sang user root Cài đặt các gói cần thiết (quá trình mất khoảng 5-10 phút): dnf install -y git tar gcc \\ zlib-devel bzip2-devel readline-devel \\ sqlite sqlite-devel openssl-devel \\ tk-devel libffi-devel xz-devel curl https://pyenv.run | bash \u0026\u0026 \\ echo 'export PYENV_ROOT=\"$HOME/.pyenv\"' \u003e\u003e ~/.bashrc \u0026\u0026 \\ echo '[[ -d $PYENV_ROOT/bin ]] \u0026\u0026 export PATH=\"$PYENV_ROOT/bin:$PATH\"' \u003e\u003e ~/.bashrc \u0026\u0026 \\ echo 'eval \"$(pyenv init -)\"' \u003e\u003e ~/.bashrc \u0026\u0026 \\ source ~/.bashrc \u0026\u0026 \\ pyenv install 3.12.4 \u0026\u0026 \\ pyenv global 3.12.4 sudo yum install postgresql17-server.x86_64 -y Kiểm tra lại phiên bản Python đã cài đặt python --version Kiểm tra lại phiên bản PostgreSQL đã cài đặt psql --version Clone repository từ github về EC2 Instance git clone --sparse --filter=blob:none https://github.com/ltdungg/aws-fashion-data-pipeline.git project cd project git sparse-checkout init git sparse-checkout set ec2 cd ec2 Cài đặt các thư viện cần thiết pip install -r requirements.txt Gắn role cho EC2 để ghi vào Kinesis Data Stream Về giao diện EC2 Instances bấm vào fashion-webapp, chọn Actions, Security, Modify IAM role. Chọn ec2-kinesis-role mà chúng ta đã tạo ở phần trước. Bấm Update IAM role.",
    "description": "Tạo Elastic Compute Cloud (EC2) Instance Truy cập Amazon Management Console Tìm kiếm dịch vụ EC2 Chọn EC2 từ kết quả tìm kiếm Chọn Launch Instance Trong giao diện Launch Instance Name: fashion-webapp Amazon Machine Image (AMI): Amazon Linux 2023 AMI Instance Type: t2.micro Tại phần Key pair, chọn Create new key pair\nKey pair name: fashion-keypair Key pair type: RSA, private key file format: .pem Chọn Create key pair Tại phần Network Settings, chọn Edit:",
    "tags": [],
    "title": "2.3 Tạo EC2 Instance",
    "uri": "/vi/preparation/setup-ec2/index.html"
  },
  {
    "breadcrumb": "AWS Fashion Pipeline",
    "content": "Tổng quan Trong phần này, ta sẽ tạo môi trường ứng dụng cho Data Pipeline. Môi trường bao gồm: EC2 Instance: Nơi tạo ra dữ liệu giả và khởi tạo các table database. RDS: Database PostgreSQL để lưu trữ dữ liệu giả. Mô hình database bao gồm 4 bảng:\nproducts, users, orders và order_details Các bước triển khai Quay lại giao diện RDS Chọn Databases từ menu bên trái Bấm vào fashion-db Tại giao diện fashion-db, lưu lại Endpoint của database vào một nơi nào đó. Kéo xuống phần Connected compute resources Bấm vào Actions Chọn Set up EC2 connection Tại giao diện Set up EC2 connection Phần EC2 Instance, chọn fashion-webapp Chọn continue Tại phần Review and confirm, chọn Set up Khi hoàn tất, bạn sẽ thấy một thông báo như sau: Quay trở lại terminal kết nối với EC2 Instance tại bước 2.3 Tạo EC2 Instance.\nTest kết nối tới database PostgreSQL với DNS Database bạn đã lưu lúc trước:\nDùng lệnh sau để kết nối tới database PostgreSQL, sau đó nhập password database bạn vừa tạo. psql -U postgres -h \u003cYOUR_POSTGRESQL_DNS\u003e -p 5432 -d postgres Nếu bạn thấy thông báo như sau thì bạn đã kết nối thành công tới database PostgreSQL.\nCòn nếu không hiện lên đúng hình ảnh phía dưới, xem lại các bước trên nhé. Gõ \\q để thoát khỏi database PostgreSQL. và trở về terminal.\nTạo môi trường cho việc tạo dữ liệu giả:\nGõ lệnh sau: vim .env Tại vim, bấm i để vào chế độ nhập liệu. Nhập các thông tin sau vào file .env: RDS_HOST: là DNS của database PostgreSQL bạn đã lưu ở bước 2. RDS_PASSWORD: là password của database PostgreSQL bạn đã tạo ở bước 2. KINESIS_STREAM_NAME: fashion-ds STREAM_ARN: Là ARN của Kinesis Stream bạn đã tạo ở bước 2.6 Tạo Kinesis Stream. Bấm Esc để thoát khỏi chế độ nhập liệu. Gõ :wq để lưu lại file .env và thoát khỏi vim. Tạo table cho database : Trong phần này, ta sẽ tạo 4 bảng cho database trên RDS Đồng thời cũng tạo ra dữ liệu giả cho 1000 users và bảng products python initdb.py Nếu bạn thấy thông báo như sau thì bạn đã tạo thành công database và dữ liệu giả cho 1000 users và bảng products. Xem các bảng đã tạo trong database PostgreSQL:\nGõ lệnh tại bước 7 để kết nối với database PostgreSQL. Gõ lệnh sau để xem các bảng đã tạo trong database PostgreSQL: SELECT * FROM information_schema.tables WHERE table_schema = 'public'; Xem tất cả dữ liệu trong bảng (Tùy chọn): SELECT * FROM \u003cTABLE_NAME\u003e; --- Ví dụ: SELECT * FROM users; Bạn đã thành công trong việc tạo môi trường ứng dụng cho Data Pipeline.",
    "description": "Tổng quan Trong phần này, ta sẽ tạo môi trường ứng dụng cho Data Pipeline. Môi trường bao gồm: EC2 Instance: Nơi tạo ra dữ liệu giả và khởi tạo các table database. RDS: Database PostgreSQL để lưu trữ dữ liệu giả. Mô hình database bao gồm 4 bảng:\nproducts, users, orders và order_details Các bước triển khai Quay lại giao diện RDS Chọn Databases từ menu bên trái Bấm vào fashion-db",
    "tags": [],
    "title": "3. Tạo môi trường ứng dụng",
    "uri": "/vi/generate-data/index.html"
  },
  {
    "breadcrumb": "AWS Fashion Pipeline \u003e  5. Test dữ liệu",
    "content": "Chạy thử Lambda Function Vào trang của Lambda Function lambda-rds-to-s3 trong AWS Console của bạn. Bấm vào Test Vì mục đích của Lambda theo yêu cầu của sếp là sau một ngày thì sẽ chạy một lần, nên đoạn code sẽ chạy vào đầu ngày hôm sau tức là lúc 00:01 ngày hôm sau. Nên muốn test thì phải thêm Event JSON và DATE_EXECUTION là ngày bạn chạy lab theo định dạng Năm-Tháng-Ngày. Ví dụ “2025-17-04” Trong phần Test: Chọn Create new event Nhập tên cho event là test Event JSON như sau: { \"DATE_EXECUTION\": \"\u003cNgày chạy workshop\u003e\" } Bấm Test\nBạn đã thành công chạy test\nVào lại S3 bucket fashion-landing-zone và kiểm tra xem đã có dữ liệu chưa nhé.",
    "description": "Chạy thử Lambda Function Vào trang của Lambda Function lambda-rds-to-s3 trong AWS Console của bạn. Bấm vào Test Vì mục đích của Lambda theo yêu cầu của sếp là sau một ngày thì sẽ chạy một lần, nên đoạn code sẽ chạy vào đầu ngày hôm sau tức là lúc 00:01 ngày hôm sau. Nên muốn test thì phải thêm Event JSON và DATE_EXECUTION là ngày bạn chạy lab theo định dạng Năm-Tháng-Ngày. Ví dụ “2025-17-04” Trong phần Test: Chọn Create new event Nhập tên cho event là test Event JSON như sau: { \"DATE_EXECUTION\": \"\u003cNgày chạy workshop\u003e\" }",
    "tags": [],
    "title": "5.3 Test Lambda Function",
    "uri": "/vi/test-connection/test-lambda/index.html"
  },
  {
    "breadcrumb": "AWS Fashion Pipeline \u003e  2. Các bước chuẩn bị",
    "content": "Cài đặt Relational Database Service (RDS) Truy cập Amazon Management Console Tìm kiếm dịch vụ RDS Chọn Aurora and RDS từ kết quả tìm kiếm Chọn Create database Tại giao diện tạo RDS, chọn Standard Create Engine options: PostgreSQL Version: PostgreSQL 17.2-R2 Templates: Free Tier DB instance identifier: fashion-db Master password: Tự đặt password cho riêng mình DB instance class: db.t4g.micro Tại phần Connectivity, chọn Edit VPC: fashion-vpc Subnet group: fashion-subnet-group Public access: No VPC security group: Chọn Create new Security group name: fashion-db-sg Availability zone: ap-southeast-1a Còn lại để mặc định và nhấn Create database để hoàn tất việc tạo RDS",
    "description": "Cài đặt Relational Database Service (RDS) Truy cập Amazon Management Console Tìm kiếm dịch vụ RDS Chọn Aurora and RDS từ kết quả tìm kiếm Chọn Create database Tại giao diện tạo RDS, chọn Standard Create Engine options: PostgreSQL Version: PostgreSQL 17.2-R2 Templates: Free Tier DB instance identifier: fashion-db Master password: Tự đặt password cho riêng mình DB instance class: db.t4g.micro Tại phần Connectivity, chọn Edit VPC: fashion-vpc Subnet group: fashion-subnet-group Public access: No VPC security group: Chọn Create new Security group name: fashion-db-sg Availability zone: ap-southeast-1a",
    "tags": [],
    "title": "2.4 Tạo RDS",
    "uri": "/vi/preparation/setup-rds/index.html"
  },
  {
    "breadcrumb": "AWS Fashion Pipeline",
    "content": "Tổng quan Trong phần này, chúng ta sẽ sử dụng AWS Lambda để trích xuất dữ liệu từ Amazon RDS và lưu vào Amazon S3. Chúng ta sẽ sử dụng một hàm Lambda đơn giản để thực hiện việc này. Hàm Lambda sẽ được kích hoạt sẽ kết nối đến Amazon RDS, thực hiện truy vấn SQL để lấy dữ liệu và sau đó lưu dữ liệu vào Amazon S3 dưới dạng file CSV. Đầu tiên tải zip file Lambda function tại đây, bấm nút download raw file góc bên phải màn hình để tải xuống. Đây là code của hàm Lambda, chúng ta sẽ không đi sâu vào chi tiết code trong phần này. Chúng ta chỉ cần biết rằng hàm Lambda này sẽ kết nối đến Amazon RDS, thực hiện truy vấn SQL và lưu dữ liệu vào Amazon S3. import psycopg2 from datetime import datetime, timedelta import boto3 import os import pandas as pd # Variables RDS_HOST = os.getenv(\"RDS_HOST\") RDS_PASSWORD = os.getenv(\"RDS_PASSWORD\") db_params = { 'dbname': 'postgres', 'user': 'postgres', 'password': RDS_PASSWORD, 'host': RDS_HOST, 'port': 5432 } DATE_EXECUTION = None # S3 RAW_BUCKET = os.getenv(\"S3_BUCKET\") s3 = boto3.client('s3') def get_table_data(table_name, query): conn = psycopg2.connect(**db_params) cursor = conn.cursor() print(f\"Getting table {table_name} data...\") # Execute the query cursor.execute(query) # Fetch all records records = cursor.fetchall() # Get column names col_names = [desc[0] for desc in cursor.description] # Convert to DataFrame df = pd.DataFrame(records, columns=col_names) print(f\"Successfully retrieved records from {table_name} table.\") return df def put_csv_to_s3(df, bucket_name, file_name): # Convert DataFrame to CSV csv_data = df.to_csv(index=False) # Upload to S3 s3.put_object(Bucket=bucket_name, Key=file_name, Body=csv_data) print(f\"File {file_name} uploaded to S3 bucket {bucket_name}\") def lambda_handler(event, context): try: DATE_EXECUTION = event[\"DATE_EXECUTION\"] except Exception as e: DATE_EXECUTION = (datetime.now() - timedelta(days=1)).strftime(\"%Y-%m-%d\") # Orders Table table_name = \"orders\" orders_query = f\"\"\" SELECT * FROM orders WHERE order_date \u003e= '{DATE_EXECUTION} 00:00:00' AND order_date \u003c= '{DATE_EXECUTION} 23:59:59' \"\"\" orders_df = get_table_data(table_name, orders_query) orders_file_name = f\"orders/orders_{DATE_EXECUTION}.csv\" put_csv_to_s3(orders_df, RAW_BUCKET, orders_file_name) # Order Details Table table_name = \"order_details\" order_details_query = f\"\"\" SELECT * FROM order_details WHERE order_id IN (SELECT id FROM orders WHERE order_date \u003e= '{DATE_EXECUTION} 00:00:00' AND order_date \u003c= '{DATE_EXECUTION} 23:59:59') \"\"\" order_details_df = get_table_data(table_name, order_details_query) order_details_file_name = f\"order_details/order_details_{DATE_EXECUTION}.csv\" put_csv_to_s3(order_details_df, RAW_BUCKET, order_details_file_name) # Users Table table_name = \"users\" users_query = f\"\"\" SELECT * FROM users \"\"\" users_df = get_table_data(table_name, users_query) users_file_name = f\"users/users_{DATE_EXECUTION}.csv\" put_csv_to_s3(users_df, RAW_BUCKET, users_file_name) # Products Table table_name = \"products\" products_query = f\"\"\" SELECT * FROM products \"\"\" products_df = get_table_data(table_name, products_query) products_file_name = f\"products/products_{DATE_EXECUTION}.csv\" put_csv_to_s3(products_df, RAW_BUCKET, products_file_name) print(\"All data has been successfully uploaded to S3.\") Truy cập vào S3 Truy cập vào AWS Console, tìm kiếm và chọn S3 từ menu dịch vụ. Bấm vào bucket fashion-logic-zone mà chúng ta đã tạo ở phần trước. Tại giao diện bucket, bấm Upload 4. Tại giao diện Upload:\nBấm Add files, chọn file rds-to-s3-lambda.zip mà chúng ta đã tải xuống ở trên. Bấm Upload. Sau khi upload thành công, chúng ta sẽ thấy file rds-to-s3-lambda.zip trong bucket. Lưu lại Object URL của file này, chúng ta sẽ sử dụng nó trong hàm Lambda. Tạo Lambda function Truy cập vào AWS Console, tìm kiếm và chọn Lambda từ menu dịch vụ. Bấm vào Create function.\nTại giao diện Create function Chọn Author from scratch. Nhập tên cho function là lambda-rds-to-s3. Chọn Python 3.13 cho Runtime. Bấm Change default execution role. Chọn Use an existing role. Chọn role lambda-s3-full-access mà chúng ta đã tạo ở phần trước. Bấm Create function. Tại giao diện Function code: Tại phần Code source, bấm vào Upload from và chọn Amazon S3 location. Nhập đường dẫn đến file rds-to-s3-lambda.zip mà chúng ta đã upload lên S3 ở phần trước. Bấm Save. Sau khi upload thành công, bấm vào Configuration Ở phần General configuration, bấm Edit. Tại phần Memory tăng giá trị lên 512 MB. Tại phần Timeout, nhập giá trị là 5 minutes. Bấm Save. Tại giao diện Configuration, bấm vào Environment variables. Bấm Edit. Bấm Add environment variable. Nhập các biến môi trường sau: RDS_HOST: Địa chỉ endpoint của Amazon RDS mà chúng ta đã tạo ở phần trước. RDS_PASSWORD: Mật khẩu của user postgres mà chúng ta đã tạo ở phần trước. S3_BUCKET: fashion-landing-zone. Bấm Save. Tại giao diện Configuration, bấm vào VPC. Chọn Edit Tại phần VPC, chọn VPC mà chúng ta đã tạo ở phần trước. Tại phần Subnets, chọn 2 subnet private mà chúng ta đã tạo ở phần trước. fashion-subnet-private1-ap-southeast-1a fashion-subnet-private2-ap-southeast-1b Select Security groups chọn default security group của VPC. Bấm Save. Cho phép Lambda truy cập vào RDS Quay lại giao diện fashion-db tại phần Databases tại giao diện RDS. Kéo xuống phần Connected compute resources, bấm vào Acion, chọn Set up Lambda connection Trong giao diện Set up Lambda connection Chọn choosing existing function Chọn hàm lambda-rds-to-s3 mà chúng ta đã tạo ở phần trước. Tắt tùy chọn Connect using Proxy Bấm Set up",
    "description": "Tổng quan Trong phần này, chúng ta sẽ sử dụng AWS Lambda để trích xuất dữ liệu từ Amazon RDS và lưu vào Amazon S3. Chúng ta sẽ sử dụng một hàm Lambda đơn giản để thực hiện việc này. Hàm Lambda sẽ được kích hoạt sẽ kết nối đến Amazon RDS, thực hiện truy vấn SQL để lấy dữ liệu và sau đó lưu dữ liệu vào Amazon S3 dưới dạng file CSV. Đầu tiên tải zip file Lambda function tại đây, bấm nút download raw file góc bên phải màn hình để tải xuống. Đây là code của hàm Lambda, chúng ta sẽ không đi sâu vào chi tiết code trong phần này. Chúng ta chỉ cần biết rằng hàm Lambda này sẽ kết nối đến Amazon RDS, thực hiện truy vấn SQL và lưu dữ liệu vào Amazon S3. import psycopg2 from datetime import datetime, timedelta import boto3 import os import pandas as pd # Variables RDS_HOST = os.getenv(\"RDS_HOST\") RDS_PASSWORD = os.getenv(\"RDS_PASSWORD\") db_params = { 'dbname': 'postgres', 'user': 'postgres', 'password': RDS_PASSWORD, 'host': RDS_HOST, 'port': 5432 } DATE_EXECUTION = None # S3 RAW_BUCKET = os.getenv(\"S3_BUCKET\") s3 = boto3.client('s3') def get_table_data(table_name, query): conn = psycopg2.connect(**db_params) cursor = conn.cursor() print(f\"Getting table {table_name} data...\") # Execute the query cursor.execute(query) # Fetch all records records = cursor.fetchall() # Get column names col_names = [desc[0] for desc in cursor.description] # Convert to DataFrame df = pd.DataFrame(records, columns=col_names) print(f\"Successfully retrieved records from {table_name} table.\") return df def put_csv_to_s3(df, bucket_name, file_name): # Convert DataFrame to CSV csv_data = df.to_csv(index=False) # Upload to S3 s3.put_object(Bucket=bucket_name, Key=file_name, Body=csv_data) print(f\"File {file_name} uploaded to S3 bucket {bucket_name}\") def lambda_handler(event, context): try: DATE_EXECUTION = event[\"DATE_EXECUTION\"] except Exception as e: DATE_EXECUTION = (datetime.now() - timedelta(days=1)).strftime(\"%Y-%m-%d\") # Orders Table table_name = \"orders\" orders_query = f\"\"\" SELECT * FROM orders WHERE order_date \u003e= '{DATE_EXECUTION} 00:00:00' AND order_date \u003c= '{DATE_EXECUTION} 23:59:59' \"\"\" orders_df = get_table_data(table_name, orders_query) orders_file_name = f\"orders/orders_{DATE_EXECUTION}.csv\" put_csv_to_s3(orders_df, RAW_BUCKET, orders_file_name) # Order Details Table table_name = \"order_details\" order_details_query = f\"\"\" SELECT * FROM order_details WHERE order_id IN (SELECT id FROM orders WHERE order_date \u003e= '{DATE_EXECUTION} 00:00:00' AND order_date \u003c= '{DATE_EXECUTION} 23:59:59') \"\"\" order_details_df = get_table_data(table_name, order_details_query) order_details_file_name = f\"order_details/order_details_{DATE_EXECUTION}.csv\" put_csv_to_s3(order_details_df, RAW_BUCKET, order_details_file_name) # Users Table table_name = \"users\" users_query = f\"\"\" SELECT * FROM users \"\"\" users_df = get_table_data(table_name, users_query) users_file_name = f\"users/users_{DATE_EXECUTION}.csv\" put_csv_to_s3(users_df, RAW_BUCKET, users_file_name) # Products Table table_name = \"products\" products_query = f\"\"\" SELECT * FROM products \"\"\" products_df = get_table_data(table_name, products_query) products_file_name = f\"products/products_{DATE_EXECUTION}.csv\" put_csv_to_s3(products_df, RAW_BUCKET, products_file_name) print(\"All data has been successfully uploaded to S3.\") Truy cập vào S3 Truy cập vào AWS Console, tìm kiếm và chọn S3 từ menu dịch vụ. Bấm vào bucket fashion-logic-zone mà chúng ta đã tạo ở phần trước. Tại giao diện bucket, bấm Upload 4. Tại giao diện Upload:",
    "tags": [],
    "title": "4. Trích xuất dữ liệu từ RDS vào S3",
    "uri": "/vi/rds-to-s3/index.html"
  },
  {
    "breadcrumb": "AWS Fashion Pipeline \u003e  2. Các bước chuẩn bị",
    "content": "Giới thiệu Trong phần này, ta sẽ tạo S3 Bucket để làm một Data Lake cho Data Pipeline. Trong phần này ta sẽ tạo các bucket sau:\nfashion-landing-zone: Đây là nơi lưu trữ dữ liệu thô chưa được xử lý. fashion-clean-zone: Đây là nơi lưu trữ dữ liệu đã được chuyển đổi và chuẩn bị sẵn sàng cho các công việc phân tích. fashion-logic-zone: Đây là nơi lưu trữ những Lambda Function và các dags chạy Airflow cho AWS MWAA. Cài đặt Truy cập Amazon Management Console Tìm kiếm dịch vụ S3 Chọn S3 từ kết quả tìm kiếm Sau đó bấm Create bucket Trong giao diện Create bucket Chọn Bucket name là fashion-landing-zone Để mặc định các thông số còn lại Bấm Create bucket Lặp lại bước 2 với các bucket sau: fashion-clean-zone fashion-logic-zone Bạn đang tạo thành công 3 bucket S3.",
    "description": "Giới thiệu Trong phần này, ta sẽ tạo S3 Bucket để làm một Data Lake cho Data Pipeline. Trong phần này ta sẽ tạo các bucket sau:\nfashion-landing-zone: Đây là nơi lưu trữ dữ liệu thô chưa được xử lý. fashion-clean-zone: Đây là nơi lưu trữ dữ liệu đã được chuyển đổi và chuẩn bị sẵn sàng cho các công việc phân tích. fashion-logic-zone: Đây là nơi lưu trữ những Lambda Function và các dags chạy Airflow cho AWS MWAA. Cài đặt Truy cập Amazon Management Console Tìm kiếm dịch vụ S3 Chọn S3 từ kết quả tìm kiếm Sau đó bấm Create bucket",
    "tags": [],
    "title": "2.5 Tạo S3 Bucket",
    "uri": "/vi/preparation/setup-s3/index.html"
  },
  {
    "breadcrumb": "AWS Fashion Pipeline",
    "content": "Tạo dữ liệu để test. Vào lại terminal kết nối với EC2 Instances. Chạy lệnh sau đi đến thư mục chứa file tạo dữ liệu: cd /home/ec2-user/project/ec2 Chạy lệnh sau để tạo dữ liệu: python data-generator.py Sau khi chạy một lúc, hãy bấm Ctrl + C để dừng lại. Ta có thể thấy dữ liệu output trên terminal như sau. Bạn có thể thấy như sau:\nOrder data insert successfully là dữ liệu order được đẩy vào RDS. Còn lại là dữ liệu tương tác khách hàng được đẩy vào Kinesis stream, với những event như page_view, add_to_cart, product_view. Bạn có thể xem code tạo dữ liệu ở bên dưới (Tùy chọn)\nCode tạo dữ liệu import faker import json import random from datetime import datetime import pandas as pd import psycopg2 from dotenv import load_dotenv import os import threading import base64 import time import boto3 # RDS Parameters load_dotenv(\".env\") RDS_HOST = os.getenv(\"RDS_HOST\") RDS_PASSWORD = os.getenv(\"RDS_PASSWORD\") LAST_ORDER_ID = None db_params = { 'dbname': 'postgres', 'user': 'postgres', 'password': RDS_PASSWORD, 'host': RDS_HOST, 'port': 5432 } # Kinesis Parameters STREAM_NAME = os.getenv(\"KINESIS_STREAM_NAME\") STREAM_ARN = os.getenv(\"STREAM_ARN\") def get_conn_and_cursor(): try: conn = psycopg2.connect(**db_params) cursor = conn.cursor() return conn, cursor except (Exception) as e: return f\"Error: {e}\" def get_last_order_id(conn, cursor): global LAST_ORDER_ID try: cursor.execute('SELECT MAX(id) FROM orders') result = cursor.fetchone() if result[0] is not None: LAST_ORDER_ID = result[0] else: LAST_ORDER_ID = None except (Exception) as e: raise f\"Error: {e}\" def get_rds_user_id(conn, cursor): try: cursor.execute('SELECT id FROM users') result = cursor.fetchall() user_id = [row[0] for row in result] return user_id except (Exception) as e: return f\"Error: {e}\" def get_rds_product(conn, cursor): try: cursor.execute('SELECT id, price FROM products') result = cursor.fetchall() product_id = [(row[0], row[1]) for row in result] return product_id except (Exception) as e: return f\"Error: {e}\" def generate_clickstream_data(user_id, product_id): event_type_enum = ['page_view', 'add_to_cart', 'product_view'] event_type = random.choice(event_type_enum) clickstream_data = { 'user_id': random.choice(user_id), 'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), 'event_type': event_type, 'product_id': random.choice(product_id) if event_type in ['add_to_cart', 'product_view'] else None } return clickstream_data def generate_order_data(user_id, products): global LAST_ORDER_ID if LAST_ORDER_ID is None: LAST_ORDER_ID = 0 else: LAST_ORDER_ID = LAST_ORDER_ID + 1 user_id = random.choice(user_id) order_date = datetime.now() numbers_of_product = random.randint(1, 10) order_details = [] random_products = random.sample(products, numbers_of_product) total_price = 0 for i in range(numbers_of_product): purchase_product = { 'order_id': int(LAST_ORDER_ID), 'product_id': int(random_products[i][0]), 'quantity': int(random.randint(1, 5)), 'unit_price': float(random_products[i][1]) } total_price += random_products[i][1] * purchase_product['quantity'] order_details.append(purchase_product) order_data = { 'id': LAST_ORDER_ID, 'user_id': user_id, 'order_date': order_date, 'total_price': total_price } order_df = pd.DataFrame(order_data, index=[0]) order_details_df = pd.DataFrame(order_details) return [order_df, order_details_df] def clickstream_task(user_id, product_id): kinesis_client = boto3.client('kinesis', region_name='ap-southeast-1') while True: clickstream_data = generate_clickstream_data(user_id, product_id) print(clickstream_data) clickstream_json = json.dumps(clickstream_data) kinesis_client.put_record( StreamName=STREAM_NAME, Data=base64.b64encode(clickstream_json.encode()), PartitionKey=str(clickstream_data['user_id']), StreamARN=STREAM_ARN ) time.sleep(random.uniform(0, 2)) def order_task(conn, cursor, user_id, products): while True: order_data = generate_order_data(user_id, products) # Insert order data into the database orders_tuple = [tuple(row) for row in order_data[0].values] order_insert_query = \"\"\" INSERT INTO orders (id, user_id, order_date, total_price) VALUES (%s, %s, %s, %s) \"\"\" cursor.executemany(order_insert_query, orders_tuple) # Insert order details data into the database order_details_tuple = [tuple(row) for row in order_data[1].values] for i in range(len(order_details_tuple)): order_details_tuple[i] = (int(order_details_tuple[i][0]), int(order_details_tuple[i][1]), int(order_details_tuple[i][2]), float(order_details_tuple[i][3])) order_details_insert_query = \"\"\" INSERT INTO order_details (order_id, product_id, quantity, unit_price) VALUES (%s, %s, %s, %s) \"\"\" cursor.executemany(order_details_insert_query, order_details_tuple) conn.commit() print(\"Order data inserted successfully. Order ID: \", order_data[0]['id'].values[0]) time.sleep(random.randint(0, 3)) if __name__ == \"__main__\": # Create database connection conn, cursor = get_conn_and_cursor() # Get last order ID get_last_order_id(conn, cursor) # Get user and product data products = get_rds_product(conn, cursor) user_id = get_rds_user_id(conn, cursor) product_id = [product[0] for product in products] # Start clickstream task clickstream_thread = threading.Thread(target=clickstream_task, args=(user_id, product_id)) clickstream_thread.start() # Start order task order_thread = threading.Thread(target=order_task, args=(conn, cursor, user_id, products)) order_thread.start()",
    "description": "Tạo dữ liệu để test. Vào lại terminal kết nối với EC2 Instances. Chạy lệnh sau đi đến thư mục chứa file tạo dữ liệu: cd /home/ec2-user/project/ec2 Chạy lệnh sau để tạo dữ liệu: python data-generator.py Sau khi chạy một lúc, hãy bấm Ctrl + C để dừng lại. Ta có thể thấy dữ liệu output trên terminal như sau. Bạn có thể thấy như sau:\nOrder data insert successfully là dữ liệu order được đẩy vào RDS. Còn lại là dữ liệu tương tác khách hàng được đẩy vào Kinesis stream, với những event như page_view, add_to_cart, product_view. Bạn có thể xem code tạo dữ liệu ở bên dưới (Tùy chọn)",
    "tags": [],
    "title": "5. Test dữ liệu",
    "uri": "/vi/test-connection/index.html"
  },
  {
    "breadcrumb": "AWS Fashion Pipeline \u003e  2. Các bước chuẩn bị",
    "content": "Tạo Kinesis Data Stream Truy cập Amazon Management Console Tìm kiếm dịch vụ Kinesis Chọn Kinesis từ kết quả tìm kiếm Trong phần giao diện Kinesis chọn Kinesis Data Streams rồi chọn Create data stream Trong giao diện Create data stream Chọn Name là fashion-ds Chọn Provisioned Phần Provisioned shards nhập 1 Chọn Create data stream Sau khi tạo xong bạn sẽ thấy giao diện như sau, lưu lại ARN của Kinesis Data Stream này để sử dụng trong các bước sau.",
    "description": "Tạo Kinesis Data Stream Truy cập Amazon Management Console Tìm kiếm dịch vụ Kinesis Chọn Kinesis từ kết quả tìm kiếm Trong phần giao diện Kinesis chọn Kinesis Data Streams rồi chọn Create data stream Trong giao diện Create data stream Chọn Name là fashion-ds Chọn Provisioned Phần Provisioned shards nhập 1 Chọn Create data stream Sau khi tạo xong bạn sẽ thấy giao diện như sau, lưu lại ARN của Kinesis Data Stream này để sử dụng trong các bước sau.",
    "tags": [],
    "title": "2.6 Tạo Kinesis Data Stream",
    "uri": "/vi/preparation/setup-kinesis/index.html"
  },
  {
    "breadcrumb": "AWS Fashion Pipeline",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Thể loại",
    "uri": "/vi/categories/index.html"
  },
  {
    "breadcrumb": "AWS Fashion Pipeline",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Thẻ",
    "uri": "/vi/tags/index.html"
  }
]
