var relearn_searchindex = [
  {
    "breadcrumb": "AWS Fashion Pipeline",
    "content": "A Data Pipeline for Fashion Store Dữ liệu Workshop này bao gồm những dữ liệu sau:\nCác sản phẩm quần áo được lấy từ Kaggle Dữ liệu người dùng, mua hàng, clickstream tự tạo bằng Faker Python Bối cảnh Giả sử bạn là Data Engineer tại một cửa hàng bán quần áo online, sếp bạn muốn biết sự quan tâm của khách hàng vào những sản phẩm nào cũng như là phân tích xem xu hướng thời trang hiện tại như thế nào.\nSếp của bạn có những yêu cầu sau:\nKhi người dùng bấm vào hoặc thêm vào giỏ hàng một sản phẩm, trực tiếp được chuyển vào kho lưu trữ, đồng thời chuyển đổi và làm giàu để ML có thể dự đoán xu hướng của thời trang hiện tại và đưa ra các gợi ý sản phẩm cho người dùng real-time. Sau đó dữ liệu gợi ý cho từng User được lưu vào nơi để có thể trực tiếp gợi ý cho người dùng ở trên webiste. Yêu cầu thứ 2 là, sau khi người dùng mua hàng, thì kết thúc một ngày, dữ liệu cần được tổng hợp, làm sạch và chuyển vào kho lưu trữ để sẵn sàng cho mục đích phân tích để tìm ra hướng cho chiến lược marketing tiếp theo. Và ngay sau đó bạn nảy ra ý tưởng triển khai trên Amazon Web Service như sau.\nÝ tưởng Bạn nghĩ ra một hệ thống kết hợp giữa Batch Processing và Real-Time Processing như sau:\nTriển khai hệ thống Database trên Amazon RDS, nơi xử lý các giao dịch mua hàng của người dùng,…\nTriển khai hệ thống Data Lake bằng Amazon S3 trên AWS để lưu trữ dữ liệu tổng hợp của cửa hàng.\nXây dựng Glue Data Catalog để có thể trực tiếp phân tích dữ liệu từ Data Lake bằng Athena, hoặc Redshift Spectrum\nĐối với luồng dữ liệu tương tác khách hàng (Clickstream), ta sẽ tạo một Kinesis Data Stream sau đó có 2 Consumer:\nDữ liệu trực tuyến đi qua Lambda, và mỗi khi người dùng tương tác sẽ kích hoạt Lambda Function gợi ý sản phẩm đẩy vào DynamoDB. Consumer còn lại đi vào Kinesis Firehose mỗi 5 phút, để đẩy vào Data Lake, nơi Data Analysis phân tích dữ liệu và đưa ra xu hướng thời trang, cũng như là doanh số,… Lên lịch cho những công việc với Amazon MWAA\nDữ liệu clickstream đi vào Lambda mỗi 5 phút. Sau một ngày dữ liệu từ RDS đi vào S3 vào 00:01 ngày hôm sau. Sau lúc bạn ghi ra những ý tưởng, đầu bạn nảy ra một hệ thống trên AWS có kiến trúc như sau\nKiến trúc hệ thống",
    "description": "A Data Pipeline for Fashion Store Dữ liệu Workshop này bao gồm những dữ liệu sau:\nCác sản phẩm quần áo được lấy từ Kaggle Dữ liệu người dùng, mua hàng, clickstream tự tạo bằng Faker Python Bối cảnh Giả sử bạn là Data Engineer tại một cửa hàng bán quần áo online, sếp bạn muốn biết sự quan tâm của khách hàng vào những sản phẩm nào cũng như là phân tích xem xu hướng thời trang hiện tại như thế nào.",
    "tags": [],
    "title": "1. Giới thiệu",
    "uri": "/vi/overview/index.html"
  },
  {
    "breadcrumb": "AWS Fashion Pipeline \u003e  2. Các bước chuẩn bị",
    "content": "Tổng quan Trong phần này chúng ta sẽ tạo các IAM Role cần thiết cho các dịch vụ AWS mà chúng ta sẽ sử dụng trong dự án này. Các role này sẽ cho phép các dịch vụ AWS truy cập vào các tài nguyên khác nhau trong tài khoản AWS.\nCác role này sẽ bao gồm:\nRole cho Lambda truy cập vào S3 Role cho EC2 truy cập vào Kinesis Role cho Lambda truy cập vào Kinesis và DynamoDB Role cho Glue truy cập vào S3 Tạo role cho Lambda truy cập vào S3 Truy cập vào AWS Console, chọn IAM từ menu dịch vụ. Chọn Roles từ menu bên trái, bấm vào Create role. Tại giao diện Select trusted entity, chọn AWS service, sau đó chọn Lambda từ danh sách dịch vụ. Bấm Next. Tại phần Add permissions\nTìm kiếm và chọn AmazonS3FullAccess từ danh sách chính sách. Tìm kiếm và chọn AWSLambdaENIManagementAccess từ danh sách chính sách. Bấm Next. Tại phần Name, review, and create\nNhập tên cho role là lambda-s3-full-access. Phần description nhập Allow Lambda access S3 with Full Access. Bấm Create role. Tạo role cho EC2 truy cập vào Kinesis Truy cập vào AWS Console, chọn IAM từ menu dịch vụ. Chọn Roles từ menu bên trái, bấm vào Create role. Tại giao diện Select trusted entity, chọn AWS service, sau đó chọn EC2 từ danh sách dịch vụ. Bấm Next. Tại phần Add permissions Tìm kiếm và chọn AmazonKinesisFullAccess từ danh sách chính sách. Bấm Next. Tại phần Name, review, and create Nhập tên cho role là ec2-kinesis-role. Phần description nhập Allow EC2 access Kinesis with Full Access. Bấm Create role. Tạo role cho Lambda truy cập vào Kinesis và DynamoDB Tạo role cho Glue truy cập vào S3",
    "description": "Tổng quan Trong phần này chúng ta sẽ tạo các IAM Role cần thiết cho các dịch vụ AWS mà chúng ta sẽ sử dụng trong dự án này. Các role này sẽ cho phép các dịch vụ AWS truy cập vào các tài nguyên khác nhau trong tài khoản AWS.\nCác role này sẽ bao gồm:\nRole cho Lambda truy cập vào S3 Role cho EC2 truy cập vào Kinesis Role cho Lambda truy cập vào Kinesis và DynamoDB Role cho Glue truy cập vào S3 Tạo role cho Lambda truy cập vào S3 Truy cập vào AWS Console, chọn IAM từ menu dịch vụ.",
    "tags": [],
    "title": "2.1 Tạo các Role cần thiết",
    "uri": "/vi/preparation/setup-role/index.html"
  },
  {
    "breadcrumb": "AWS Fashion Pipeline \u003e  5. Test dữ liệu",
    "content": "Kiểm tra dữ liệu trong RDS. Sử dụng lệnh truy cập vào PostgreSQL ở bước trước. psql -U postgres -h \u003cYOUR_POSTGRESQL_DNS\u003e -p 5432 -d postgres Nếu bạn thấy thông báo như sau thì bạn đã kết nối thành công tới database PostgreSQL. Chạy lệnh sau để kiểm tra dữ liệu trong bảng orders:\nSELECT * FROM orders; Chạy lệnh sau để kiểm tra dữ liệu trong bảng order_details: SELECT * FROM order_details; Dưới đây là hình ảnh bảng orders ví dụ, có thể dữ liệu sẽ khác nhau do random. Dưới đây là hình ảnh bảng order_details ví dụ, có thể dữ liệu sẽ khác nhau do random. Gõ \\q để thoát khỏi database PostgreSQL. và trở về terminal. Vậy là bạn đã hoàn tất việc kiểm tra dữ liệu trong RDS. Bạn có thể sử dụng các câu lệnh SQL khác để kiểm tra dữ liệu trong các bảng khác như users, products hoặc thực hiện các truy vấn phức tạp hơn.",
    "description": "Kiểm tra dữ liệu trong RDS. Sử dụng lệnh truy cập vào PostgreSQL ở bước trước. psql -U postgres -h \u003cYOUR_POSTGRESQL_DNS\u003e -p 5432 -d postgres Nếu bạn thấy thông báo như sau thì bạn đã kết nối thành công tới database PostgreSQL. Chạy lệnh sau để kiểm tra dữ liệu trong bảng orders:\nSELECT * FROM orders; Chạy lệnh sau để kiểm tra dữ liệu trong bảng order_details: SELECT * FROM order_details; Dưới đây là hình ảnh bảng orders ví dụ, có thể dữ liệu sẽ khác nhau do random.",
    "tags": [],
    "title": "5.1 Kiểm tra dữ liệu trong RDS",
    "uri": "/vi/test-connection/test-rds/index.html"
  },
  {
    "breadcrumb": "AWS Fashion Pipeline \u003e  6. Chuyển đổi dữ liệu",
    "content": "Tổng quan Trong phần này chúng ta sẽ tạo Glue Job để chuyển đổi dữ liệu từ S3 bucket fashion-landing-zone sang S3 bucket fashion-clean-zone.\nTạo Glue Job Đăng nhập vào AWS Management Console và tìm kiếm dịch vụ AWS Glue. Tại giao diện AWS Glue, chọn “Go to ETL jobs” từ menu bên trái Tại giao diện ETL jobs, chọn Create job from a blank graph hoặc Visual ETL để tạo Glue Job mới Tại giao diện tạo Glue Job mới, bấm vào Script sau đó bấm vào Edit Script chọn Confirm Sao chép mã từ đường dẫn sau và dán vào ô Script: Glue Job Script Sau đó tại dòng thứ 13, điền tên S3 bucket của bạn vào biến LANDING_BUCKET Tại dòng thứ 14, điền tên S3 bucket của bạn vào biến CLEAN_BUCKET Sau đó bấm vào Job details. Tại giao diện Job details, điền các thông tin sau: Name: fashion-datalake-etl IAM role: Chọn IAM Role “AWS-Glue-S3-Full-Access” mà bạn đã tạo ở phần trước. Kéo xuống dưới cùng, bấm vào Advanced properties Tại Script path: Nhập đường dẫn như sau s3://\u003cYOUR-BUCKET-LOGIC-ZONE\u003e/fashion-datalake-etl/script/ Tại Spark UI logs path: Nhập đường dẫn như sau s3://\u003cYOUR-BUCKET-LOGIC-ZONE\u003e/fashion-datalake-etl/sparkHistoryLogs/ Tại Temporary path: Nhập đường dẫn như sau s3://\u003cYOUR-BUCKET-LOGIC-ZONE\u003e/fashion-datalake-etl/temporary/ Bấm Save để lưu lại Glue Job. Lên lịch chạy Glue Job tự động sau một ngày: Tại giao diện Glue Job, chọn Schedules từ menu bên trái Bấm Create schedule Tại giao diện tạo lịch chạy Glue Job vào 0:00 hằng ngày, điền các thông tin sau: Name: fashion-datalake-etl-schedule Frequency: Chọn Daily Start hour: Nhập 17 vì lịch chạy theo giờ UTC, nên cần -7 giờ để chạy vào 0:00 giờ theo giờ Việt Nam. Minute of the hour: Nhập 0 Chọn Create schedule để tạo lịch chạy Glue Job Chạy Glue Job Tại giao diện Glue Job, chọn Run, sau đó chọn Runs ở menu bên trái để kiểm tra trạng thái chạy Glue Job. Đợi một chút để Glue Job chạy xong, sau đó kiểm tra trạng thái Glue Job đã chạy thành công hay chưa. Nếu trạng thái là Succeeded thì Glue Job đã chạy thành công. Kiểm tra dữ liệu trong S3 bucket fashion-clean-zone đã có dữ liệu hay chưa. Nếu có dữ liệu thì Glue Job đã chạy thành công.\nKiểm tra output của Glue Job ở trong CloudWatch Logs\nVào dịch vụ CloudWatch, chọn Log groups Sau đó tìm kiếm log group có tên là /aws-glue/jobs/output và bấm vào Tại đây bạn sẽ thấy các log của Glue Job mà bạn đã chạy.",
    "description": "Tổng quan Trong phần này chúng ta sẽ tạo Glue Job để chuyển đổi dữ liệu từ S3 bucket fashion-landing-zone sang S3 bucket fashion-clean-zone.\nTạo Glue Job Đăng nhập vào AWS Management Console và tìm kiếm dịch vụ AWS Glue. Tại giao diện AWS Glue, chọn “Go to ETL jobs” từ menu bên trái Tại giao diện ETL jobs, chọn Create job from a blank graph hoặc Visual ETL để tạo Glue Job mới",
    "tags": [],
    "title": "6.1 Tạo Glue Job",
    "uri": "/vi/transform/transform/index.html"
  },
  {
    "breadcrumb": "AWS Fashion Pipeline",
    "content": "Tổng quan Đầu tiên là tạo một môi trường ảo, coi như đây là môi trường ứng dụng của cửa hàng. EC2 Instance sẽ là máy chủ giả, nơi tạo ra dữ liệu cho lab. RDS Instance sẽ là cơ sở dữ liệu, dành cho các giao dịch… S3 Bucket sẽ là Data Lake, bao gồm các bucket sau: fashion-landing-zone: Nơi lưu trữ dữ liệu thô từ các nguồn khác nhau. fashion-clean-zone: Nơi lưu trữ dữ liệu đã được xử lý và chuẩn bị cho việc phân tích. fashion-logic-zone: Nơi lưu trữ các script Lambda và mã nguồn cho các quy trình ETL (Extract, Transform, Load). Kinesis Stream sẽ là luồng lưu trữ các sự kiện tương tác của khách hàng Các bước triển khai Đảm bảo để Region là Singapore (ap-southeast-1) trước khi thực hiện các bước sau:\n2.1 Tạo các Role cần thiết\n2.2 Tạo VPC, Subnet, …\n2.3 Tạo EC2 Instance\n2.4 Tạo RDS Instance\n2.5 Tạo S3 Bucket\n2.6 Tạo Kinesis Stream",
    "description": "Tổng quan Đầu tiên là tạo một môi trường ảo, coi như đây là môi trường ứng dụng của cửa hàng. EC2 Instance sẽ là máy chủ giả, nơi tạo ra dữ liệu cho lab. RDS Instance sẽ là cơ sở dữ liệu, dành cho các giao dịch… S3 Bucket sẽ là Data Lake, bao gồm các bucket sau: fashion-landing-zone: Nơi lưu trữ dữ liệu thô từ các nguồn khác nhau. fashion-clean-zone: Nơi lưu trữ dữ liệu đã được xử lý và chuẩn bị cho việc phân tích. fashion-logic-zone: Nơi lưu trữ các script Lambda và mã nguồn cho các quy trình ETL (Extract, Transform, Load). Kinesis Stream sẽ là luồng lưu trữ các sự kiện tương tác của khách hàng Các bước triển khai Đảm bảo để Region là Singapore (ap-southeast-1) trước khi thực hiện các bước sau:",
    "tags": [],
    "title": "2. Các bước chuẩn bị",
    "uri": "/vi/preparation/index.html"
  },
  {
    "breadcrumb": "AWS Fashion Pipeline \u003e  2. Các bước chuẩn bị",
    "content": "Tạo VPC (Virtual Private Cloud) Truy cập Amazon Management Console Tìm kiếm dịch vụ VPC Chọn VPC từ kết quả tìm kiếm Trong phần giao diện VPC chọn Create VPC Trong giao diện Create VPC Chọn VPC and more name tag auto-generation ghi: fashion IPv4 CIDR block: 10.10.0.0/16 Còn lại để mặc định, chọn Create VPC ở phía bên dưới màn hình.",
    "description": "Tạo VPC (Virtual Private Cloud) Truy cập Amazon Management Console Tìm kiếm dịch vụ VPC Chọn VPC từ kết quả tìm kiếm Trong phần giao diện VPC chọn Create VPC Trong giao diện Create VPC Chọn VPC and more name tag auto-generation ghi: fashion IPv4 CIDR block: 10.10.0.0/16 Còn lại để mặc định, chọn Create VPC ở phía bên dưới màn hình.",
    "tags": [],
    "title": "2.2 Tạo VPC, Subnet, ...",
    "uri": "/vi/preparation/setup-env/index.html"
  },
  {
    "breadcrumb": "AWS Fashion Pipeline \u003e  5. Test dữ liệu",
    "content": "Kiểm tra dữ liệu trong Kinesis Data Stream Quay trở lại terminal kết nối với EC2 Instance tại bước 2.3 Tạo EC2 Instance. Chạy lệnh sau để mô tả luồng Kinesis Data Stream: aws kinesis describe-stream --stream-name fashion-ds Chạy lệnh sau để lấy iterator của shard trong luồng Kinesis Data Stream: aws kinesis get-shard-iterator --stream-name fashion-ds --shard-id shardId-000000000000 --shard-iterator-type TRIM_HORIZON Copy ShardIterator trong kết quả trả về của lệnh trên và chạy lệnh sau để lấy dữ liệu trong luồng Kinesis Data Stream: aws kinesis get-records --shard-iterator \u003cYOUR_SHARD_ITERATOR\u003e Trong phần Records:\nPartitionKey là user ID Data là những dữ liệu JSON được encode trong luồng Kiểm tra dữ liệu, copy một Data bên trong Records. Vào trang Base64 Decode để decode dữ liệu vừa copy.\nDecode UTF-8",
    "description": "Kiểm tra dữ liệu trong Kinesis Data Stream Quay trở lại terminal kết nối với EC2 Instance tại bước 2.3 Tạo EC2 Instance. Chạy lệnh sau để mô tả luồng Kinesis Data Stream: aws kinesis describe-stream --stream-name fashion-ds Chạy lệnh sau để lấy iterator của shard trong luồng Kinesis Data Stream: aws kinesis get-shard-iterator --stream-name fashion-ds --shard-id shardId-000000000000 --shard-iterator-type TRIM_HORIZON Copy ShardIterator trong kết quả trả về của lệnh trên và chạy lệnh sau để lấy dữ liệu trong luồng Kinesis Data Stream: aws kinesis get-records --shard-iterator \u003cYOUR_SHARD_ITERATOR\u003e",
    "tags": [],
    "title": "5.2 Kiểm tra dữ liệu trong Kinesis Data Stream",
    "uri": "/vi/test-connection/test-kinesis/index.html"
  },
  {
    "breadcrumb": "AWS Fashion Pipeline \u003e  6. Chuyển đổi dữ liệu",
    "content": "Tạo Glue Data Catalog cho Landing Zone Truy cập Amazon Management Console Tìm kiếm dịch vụ Glue Chọn Glue từ kết quả tìm kiếm Tạo Database cho Glue Data Catalog Trong phần Glue Data Catalog chọn Databases rồi chọn Add database Nhập tên database là fashion-clean-zone Chọn Create Tạo table cho Glue Data Catalog Bấm vào database fashion-clean-zone vừa tạo Chọn Add table rồi chọn Add tables using a crawler Trong phần crawler properties Nhập tên crawler là fashion-clean-zone-crawler Chọn Next Chọn nguồn dữ liệu Chọn S3 rồi chọn Browse Chọn bucket fashion-clean-zone rồi chọn Add Chọn Next",
    "description": "Tạo Glue Data Catalog cho Landing Zone Truy cập Amazon Management Console Tìm kiếm dịch vụ Glue Chọn Glue từ kết quả tìm kiếm Tạo Database cho Glue Data Catalog Trong phần Glue Data Catalog chọn Databases rồi chọn Add database Nhập tên database là fashion-clean-zone Chọn Create Tạo table cho Glue Data Catalog Bấm vào database fashion-clean-zone vừa tạo Chọn Add table rồi chọn Add tables using a crawler",
    "tags": [],
    "title": "6.2 Tạo Catalog cho Clean Zone",
    "uri": "/vi/transform/glue-catalog/index.html"
  },
  {
    "breadcrumb": "AWS Fashion Pipeline \u003e  2. Các bước chuẩn bị",
    "content": "Tổng quan Ở bước này, chúng ta sẽ tạo một EC2 Instance để chạy giả lập một ứng dụng Website bán hàng. Và trong phần này sẽ tạo EC2 Instance và cài đặt môi trường cho ứng dụng bao gồm:\nMôi trường Python Môi trường PostgreSQL Môi trường Git Và cài đặt các thư viện cần thiết cho ứng dụng Tạo Elastic Compute Cloud (EC2) Instance Truy cập Amazon Management Console Tìm kiếm dịch vụ EC2 Chọn EC2 từ kết quả tìm kiếm Chọn Launch Instance Trong giao diện Launch Instance Name: fashion-webapp Amazon Machine Image (AMI): Amazon Linux 2023 AMI Instance Type: t2.micro Tại phần Key pair, chọn Create new key pair\nKey pair name: fashion-keypair Key pair type: RSA, private key file format: .pem Chọn Create key pair Tại phần Network Settings, chọn Edit:\nVPC: fashion-vpc Subnet: fashion-subnet-public1-ap-southeast-1a Auto-assign Public IP: Enable Security Group chọn Create security group security group name: fashion-webapp-sg description: Allow SSH from My IP, and HTTP, HTTPS from everywhere Tại Inbound Security Group Rules, cài đặt như hình sau Còn lại để mặc định, nhấn Launch instance để hoàn tất việc tạo EC2 Instance Cài đặt môi trường bên trong EC2 Instance Kết nối SSH vào EC2 Instance Di chuyển tới thư mục chứa file fashion-keypair.pem Sử dụng terminal, gõ lệnh sau: ssh -i fashion-keypair.pem ec2-user@\u003cPublic IP | Public DNS\u003e Gõ ‘yes’ Nếu không kết nối được, hãy kiểm tra lại Security Group đã cho phép SSH từ IP của bạn chưa nhé. Cài đặt môi trường Python, Git, PostgreSQL cho EC2 Instance Đầu tiên, gõ sudo su để chuyển sang user root Cài đặt các gói cần thiết (quá trình mất khoảng 5-10 phút): dnf install -y git tar gcc \\ zlib-devel bzip2-devel readline-devel \\ sqlite sqlite-devel openssl-devel \\ tk-devel libffi-devel xz-devel curl https://pyenv.run | bash \u0026\u0026 \\ echo 'export PYENV_ROOT=\"$HOME/.pyenv\"' \u003e\u003e ~/.bashrc \u0026\u0026 \\ echo '[[ -d $PYENV_ROOT/bin ]] \u0026\u0026 export PATH=\"$PYENV_ROOT/bin:$PATH\"' \u003e\u003e ~/.bashrc \u0026\u0026 \\ echo 'eval \"$(pyenv init -)\"' \u003e\u003e ~/.bashrc \u0026\u0026 \\ source ~/.bashrc \u0026\u0026 \\ pyenv install 3.12.4 \u0026\u0026 \\ pyenv global 3.12.4 sudo yum install postgresql17-server.x86_64 -y Kiểm tra lại phiên bản Python đã cài đặt python --version Kiểm tra lại phiên bản PostgreSQL đã cài đặt psql --version Clone repository từ github về EC2 Instance git clone --sparse --filter=blob:none https://github.com/ltdungg/aws-fashion-data-pipeline.git project cd project git sparse-checkout init git sparse-checkout set ec2 cd ec2 Cài đặt các thư viện cần thiết pip install -r requirements.txt Gắn role cho EC2 để ghi vào Kinesis Data Stream Về giao diện EC2 Instances bấm vào fashion-webapp, chọn Actions, Security, Modify IAM role. Chọn ec2-kinesis-role mà chúng ta đã tạo ở phần trước. Bấm Update IAM role.",
    "description": "Tổng quan Ở bước này, chúng ta sẽ tạo một EC2 Instance để chạy giả lập một ứng dụng Website bán hàng. Và trong phần này sẽ tạo EC2 Instance và cài đặt môi trường cho ứng dụng bao gồm:\nMôi trường Python Môi trường PostgreSQL Môi trường Git Và cài đặt các thư viện cần thiết cho ứng dụng Tạo Elastic Compute Cloud (EC2) Instance Truy cập Amazon Management Console Tìm kiếm dịch vụ EC2 Chọn EC2 từ kết quả tìm kiếm",
    "tags": [],
    "title": "2.3 Tạo EC2 Instance",
    "uri": "/vi/preparation/setup-ec2/index.html"
  },
  {
    "breadcrumb": "AWS Fashion Pipeline",
    "content": "Tổng quan Trong phần này, ta sẽ tạo môi trường ứng dụng cho Data Pipeline. Môi trường bao gồm: EC2 Instance: Nơi tạo ra dữ liệu giả và khởi tạo các table database. RDS: Database PostgreSQL để lưu trữ dữ liệu giả. Mô hình database bao gồm 4 bảng:\nproducts, users, orders và order_details Các bước triển khai Quay lại giao diện RDS Chọn Databases từ menu bên trái Bấm vào fashion-db Tại giao diện fashion-db, lưu lại Endpoint của database vào một nơi nào đó. Kéo xuống phần Connected compute resources Bấm vào Actions Chọn Set up EC2 connection Tại giao diện Set up EC2 connection Phần EC2 Instance, chọn fashion-webapp Chọn continue Tại phần Review and confirm, chọn Set up Khi hoàn tất, bạn sẽ thấy một thông báo như sau: Quay trở lại terminal kết nối với EC2 Instance tại bước 2.3 Tạo EC2 Instance.\nTest kết nối tới database PostgreSQL với DNS Database bạn đã lưu lúc trước:\nDùng lệnh sau để kết nối tới database PostgreSQL, sau đó nhập password database bạn vừa tạo. psql -U postgres -h \u003cYOUR_POSTGRESQL_DNS\u003e -p 5432 -d postgres Nếu bạn thấy thông báo như sau thì bạn đã kết nối thành công tới database PostgreSQL.\nCòn nếu không hiện lên đúng hình ảnh phía dưới, xem lại các bước trên nhé. Gõ \\q để thoát khỏi database PostgreSQL. và trở về terminal.\nTạo môi trường cho việc tạo dữ liệu giả:\nGõ lệnh sau: vim .env Tại vim, bấm i để vào chế độ nhập liệu. Nhập các thông tin sau vào file .env: RDS_HOST: là DNS của database PostgreSQL bạn đã lưu ở bước 2. RDS_PASSWORD: là password của database PostgreSQL bạn đã tạo ở bước 2. KINESIS_STREAM_NAME: fashion-ds STREAM_ARN: Là ARN của Kinesis Stream bạn đã tạo ở bước 2.6 Tạo Kinesis Stream. Bấm Esc để thoát khỏi chế độ nhập liệu. Gõ :wq để lưu lại file .env và thoát khỏi vim. Tạo table cho database : Trong phần này, ta sẽ tạo 4 bảng cho database trên RDS Đồng thời cũng tạo ra dữ liệu giả cho 1000 users và bảng products python initdb.py Nếu bạn thấy thông báo như sau thì bạn đã tạo thành công database và dữ liệu giả cho 1000 users và bảng products. Xem các bảng đã tạo trong database PostgreSQL:\nGõ lệnh tại bước 7 để kết nối với database PostgreSQL. Gõ lệnh sau để xem các bảng đã tạo trong database PostgreSQL: SELECT * FROM information_schema.tables WHERE table_schema = 'public'; Xem tất cả dữ liệu trong bảng (Tùy chọn): SELECT * FROM \u003cTABLE_NAME\u003e; --- Ví dụ: SELECT * FROM users; Bạn đã thành công trong việc tạo môi trường ứng dụng cho Data Pipeline.",
    "description": "Tổng quan Trong phần này, ta sẽ tạo môi trường ứng dụng cho Data Pipeline. Môi trường bao gồm: EC2 Instance: Nơi tạo ra dữ liệu giả và khởi tạo các table database. RDS: Database PostgreSQL để lưu trữ dữ liệu giả. Mô hình database bao gồm 4 bảng:\nproducts, users, orders và order_details Các bước triển khai Quay lại giao diện RDS Chọn Databases từ menu bên trái Bấm vào fashion-db",
    "tags": [],
    "title": "3. Tạo môi trường ứng dụng",
    "uri": "/vi/generate-data/index.html"
  },
  {
    "breadcrumb": "AWS Fashion Pipeline \u003e  5. Test dữ liệu",
    "content": "Chạy thử Lambda Function Vào trang của Lambda Function lambda-rds-to-s3 trong AWS Console của bạn. Bấm vào Test Vì mục đích của Lambda theo yêu cầu của sếp là sau một ngày thì sẽ chạy một lần, nên đoạn code sẽ chạy vào đầu ngày hôm sau tức là lúc 00:01 ngày hôm sau. Nên muốn test thì phải thêm Event JSON và DATE_EXECUTION là ngày bạn chạy lab theo định dạng Năm-Tháng-Ngày. Ví dụ “2025-17-04” Trong phần Test: Chọn Create new event Nhập tên cho event là test Event JSON như sau: { \"DATE_EXECUTION\": \"\u003cNgày chạy workshop\u003e\" } Bấm Test\nBạn đã thành công chạy test\nVào lại S3 bucket fashion-landing-zone và kiểm tra xem đã có dữ liệu chưa nhé.",
    "description": "Chạy thử Lambda Function Vào trang của Lambda Function lambda-rds-to-s3 trong AWS Console của bạn. Bấm vào Test Vì mục đích của Lambda theo yêu cầu của sếp là sau một ngày thì sẽ chạy một lần, nên đoạn code sẽ chạy vào đầu ngày hôm sau tức là lúc 00:01 ngày hôm sau. Nên muốn test thì phải thêm Event JSON và DATE_EXECUTION là ngày bạn chạy lab theo định dạng Năm-Tháng-Ngày. Ví dụ “2025-17-04” Trong phần Test: Chọn Create new event Nhập tên cho event là test Event JSON như sau: { \"DATE_EXECUTION\": \"\u003cNgày chạy workshop\u003e\" }",
    "tags": [],
    "title": "5.3 Test Lambda Function",
    "uri": "/vi/test-connection/test-lambda/index.html"
  },
  {
    "breadcrumb": "AWS Fashion Pipeline \u003e  2. Các bước chuẩn bị",
    "content": "Cài đặt Relational Database Service (RDS) Truy cập Amazon Management Console Tìm kiếm dịch vụ RDS Chọn Aurora and RDS từ kết quả tìm kiếm Chọn Create database Tại giao diện tạo RDS, chọn Standard Create Engine options: PostgreSQL Version: PostgreSQL 17.2-R2 Templates: Free Tier DB instance identifier: fashion-db Master password: Tự đặt password cho riêng mình DB instance class: db.t4g.micro Tại phần Connectivity, chọn Edit VPC: fashion-vpc Subnet group: fashion-subnet-group Public access: No VPC security group: Chọn Create new Security group name: fashion-db-sg Availability zone: ap-southeast-1a Còn lại để mặc định và nhấn Create database để hoàn tất việc tạo RDS",
    "description": "Cài đặt Relational Database Service (RDS) Truy cập Amazon Management Console Tìm kiếm dịch vụ RDS Chọn Aurora and RDS từ kết quả tìm kiếm Chọn Create database Tại giao diện tạo RDS, chọn Standard Create Engine options: PostgreSQL Version: PostgreSQL 17.2-R2 Templates: Free Tier DB instance identifier: fashion-db Master password: Tự đặt password cho riêng mình DB instance class: db.t4g.micro Tại phần Connectivity, chọn Edit VPC: fashion-vpc Subnet group: fashion-subnet-group Public access: No VPC security group: Chọn Create new Security group name: fashion-db-sg Availability zone: ap-southeast-1a",
    "tags": [],
    "title": "2.4 Tạo RDS",
    "uri": "/vi/preparation/setup-rds/index.html"
  },
  {
    "breadcrumb": "AWS Fashion Pipeline",
    "content": "Tổng quan Trong phần này, chúng ta sẽ sử dụng AWS Lambda để trích xuất dữ liệu từ Amazon RDS và lưu vào Amazon S3. Chúng ta sẽ sử dụng một hàm Lambda đơn giản để thực hiện việc này. Hàm Lambda sẽ được kích hoạt sẽ kết nối đến Amazon RDS, thực hiện truy vấn SQL để lấy dữ liệu và sau đó lưu dữ liệu vào Amazon S3 dưới dạng file CSV. Đầu tiên tải zip file Lambda function tại đây, bấm nút download raw file góc bên phải màn hình để tải xuống. Truy cập vào S3 Truy cập vào AWS Console, tìm kiếm và chọn S3 từ menu dịch vụ. Bấm vào bucket fashion-logic-zone mà chúng ta đã tạo ở phần trước. Tại giao diện bucket, bấm Upload 4. Tại giao diện Upload:\nBấm Add files, chọn file rds-to-s3-lambda.zip mà chúng ta đã tải xuống ở trên. Bấm Upload. Sau khi upload thành công, chúng ta sẽ thấy file rds-to-s3-lambda.zip trong bucket. Lưu lại Object URL của file này, chúng ta sẽ sử dụng nó trong hàm Lambda. Tạo Lambda function Truy cập vào AWS Console, tìm kiếm và chọn Lambda từ menu dịch vụ. Bấm vào Create function.\nTại giao diện Create function Chọn Author from scratch. Nhập tên cho function là lambda-rds-to-s3. Chọn Python 3.13 cho Runtime. Bấm Change default execution role. Chọn Use an existing role. Chọn role lambda-s3-full-access mà chúng ta đã tạo ở phần trước. Bấm Create function. Tại giao diện Function code: Tại phần Code source, bấm vào Upload from và chọn Amazon S3 location. Nhập đường dẫn đến file rds-to-s3-lambda.zip mà chúng ta đã upload lên S3 ở phần trước. Bấm Save. Sau khi upload thành công, bấm vào Configuration Ở phần General configuration, bấm Edit. Tại phần Memory tăng giá trị lên 512 MB. Tại phần Timeout, nhập giá trị là 5 minutes. Bấm Save. Tại giao diện Configuration, bấm vào Environment variables. Bấm Edit. Bấm Add environment variable. Nhập các biến môi trường sau: RDS_HOST: Địa chỉ endpoint của Amazon RDS mà chúng ta đã tạo ở phần trước. RDS_PASSWORD: Mật khẩu của user postgres mà chúng ta đã tạo ở phần trước. S3_BUCKET: fashion-landing-zone. Bấm Save. Tại giao diện Configuration, bấm vào VPC. Chọn Edit Tại phần VPC, chọn VPC mà chúng ta đã tạo ở phần trước. Tại phần Subnets, chọn 2 subnet private mà chúng ta đã tạo ở phần trước. fashion-subnet-private1-ap-southeast-1a fashion-subnet-private2-ap-southeast-1b Select Security groups chọn default security group của VPC. Bấm Save. Cho phép Lambda truy cập vào RDS Quay lại giao diện fashion-db tại phần Databases tại giao diện RDS. Kéo xuống phần Connected compute resources, bấm vào Acion, chọn Set up Lambda connection Trong giao diện Set up Lambda connection Chọn choosing existing function Chọn hàm lambda-rds-to-s3 mà chúng ta đã tạo ở phần trước. Tắt tùy chọn Connect using Proxy Bấm Set up",
    "description": "Tổng quan Trong phần này, chúng ta sẽ sử dụng AWS Lambda để trích xuất dữ liệu từ Amazon RDS và lưu vào Amazon S3. Chúng ta sẽ sử dụng một hàm Lambda đơn giản để thực hiện việc này. Hàm Lambda sẽ được kích hoạt sẽ kết nối đến Amazon RDS, thực hiện truy vấn SQL để lấy dữ liệu và sau đó lưu dữ liệu vào Amazon S3 dưới dạng file CSV. Đầu tiên tải zip file Lambda function tại đây, bấm nút download raw file góc bên phải màn hình để tải xuống. Truy cập vào S3 Truy cập vào AWS Console, tìm kiếm và chọn S3 từ menu dịch vụ. Bấm vào bucket fashion-logic-zone mà chúng ta đã tạo ở phần trước. Tại giao diện bucket, bấm Upload 4. Tại giao diện Upload:",
    "tags": [],
    "title": "4. Trích xuất dữ liệu từ RDS vào S3",
    "uri": "/vi/rds-to-s3/index.html"
  },
  {
    "breadcrumb": "AWS Fashion Pipeline \u003e  2. Các bước chuẩn bị",
    "content": "Giới thiệu Trong phần này, ta sẽ tạo S3 Bucket để làm một Data Lake cho Data Pipeline. Trong phần này ta sẽ tạo các bucket sau:\nfashion-landing-zone: Đây là nơi lưu trữ dữ liệu thô chưa được xử lý. fashion-clean-zone: Đây là nơi lưu trữ dữ liệu đã được chuyển đổi và chuẩn bị sẵn sàng cho các công việc phân tích. fashion-logic-zone: Đây là nơi lưu trữ những Lambda Function và các dags chạy Airflow cho AWS MWAA. Cài đặt Truy cập Amazon Management Console Tìm kiếm dịch vụ S3 Chọn S3 từ kết quả tìm kiếm Sau đó bấm Create bucket Trong giao diện Create bucket Chọn Bucket name là fashion-landing-zone Để mặc định các thông số còn lại Bấm Create bucket Lặp lại bước 2 với các bucket sau: fashion-clean-zone fashion-logic-zone Bạn đang tạo thành công 3 bucket S3.",
    "description": "Giới thiệu Trong phần này, ta sẽ tạo S3 Bucket để làm một Data Lake cho Data Pipeline. Trong phần này ta sẽ tạo các bucket sau:\nfashion-landing-zone: Đây là nơi lưu trữ dữ liệu thô chưa được xử lý. fashion-clean-zone: Đây là nơi lưu trữ dữ liệu đã được chuyển đổi và chuẩn bị sẵn sàng cho các công việc phân tích. fashion-logic-zone: Đây là nơi lưu trữ những Lambda Function và các dags chạy Airflow cho AWS MWAA. Cài đặt Truy cập Amazon Management Console Tìm kiếm dịch vụ S3 Chọn S3 từ kết quả tìm kiếm Sau đó bấm Create bucket",
    "tags": [],
    "title": "2.5 Tạo S3 Bucket",
    "uri": "/vi/preparation/setup-s3/index.html"
  },
  {
    "breadcrumb": "AWS Fashion Pipeline",
    "content": "Tạo dữ liệu để test. Vào lại terminal kết nối với EC2 Instances. Chạy lệnh sau đi đến thư mục chứa file tạo dữ liệu: cd /home/ec2-user/project/ec2 Chạy lệnh sau để tạo dữ liệu: python data-generator.py Sau khi chạy một lúc (để khoảng 5 phút), hãy bấm Ctrl + C để dừng lại. Ta có thể thấy dữ liệu output trên terminal như sau. Bạn có thể thấy như sau: Order data insert successfully là dữ liệu order được đẩy vào RDS. Còn lại là dữ liệu tương tác khách hàng được đẩy vào Kinesis stream, với những event như page_view, add_to_cart, product_view.",
    "description": "Tạo dữ liệu để test. Vào lại terminal kết nối với EC2 Instances. Chạy lệnh sau đi đến thư mục chứa file tạo dữ liệu: cd /home/ec2-user/project/ec2 Chạy lệnh sau để tạo dữ liệu: python data-generator.py Sau khi chạy một lúc (để khoảng 5 phút), hãy bấm Ctrl + C để dừng lại. Ta có thể thấy dữ liệu output trên terminal như sau. Bạn có thể thấy như sau: Order data insert successfully là dữ liệu order được đẩy vào RDS. Còn lại là dữ liệu tương tác khách hàng được đẩy vào Kinesis stream, với những event như page_view, add_to_cart, product_view.",
    "tags": [],
    "title": "5. Test dữ liệu",
    "uri": "/vi/test-connection/index.html"
  },
  {
    "breadcrumb": "AWS Fashion Pipeline \u003e  2. Các bước chuẩn bị",
    "content": "Tạo Kinesis Data Stream Truy cập Amazon Management Console Tìm kiếm dịch vụ Kinesis Chọn Kinesis từ kết quả tìm kiếm Trong phần giao diện Kinesis chọn Kinesis Data Streams rồi chọn Create data stream Trong giao diện Create data stream Chọn Name là fashion-ds Chọn Provisioned Phần Provisioned shards nhập 1 Chọn Create data stream Sau khi tạo xong bạn sẽ thấy giao diện như sau, lưu lại ARN của Kinesis Data Stream này để sử dụng trong các bước sau.",
    "description": "Tạo Kinesis Data Stream Truy cập Amazon Management Console Tìm kiếm dịch vụ Kinesis Chọn Kinesis từ kết quả tìm kiếm Trong phần giao diện Kinesis chọn Kinesis Data Streams rồi chọn Create data stream Trong giao diện Create data stream Chọn Name là fashion-ds Chọn Provisioned Phần Provisioned shards nhập 1 Chọn Create data stream Sau khi tạo xong bạn sẽ thấy giao diện như sau, lưu lại ARN của Kinesis Data Stream này để sử dụng trong các bước sau.",
    "tags": [],
    "title": "2.6 Tạo Kinesis Data Stream",
    "uri": "/vi/preparation/setup-kinesis/index.html"
  },
  {
    "breadcrumb": "AWS Fashion Pipeline",
    "content": "Các bước thực hiện Trong phần này chúng ta sẽ sử dụng Glue ETL để thực hiện chuyển đổi dữ liệu từ S3 bucket fashion-landing-zone sang S3 bucket fashion-clean-zone. Với mục đích sẵn sàng cho việc phân tích dữ liệu cũng như lưu trữ dữ liệu.\nPhần này sẽ bao gồm các bước sau:\nTạo Glue Job để chuyển đổi dữ liệu từ S3 bucket fashion-landing-zone sang S3 bucket fashion-clean-zone Tạo Glue Trigger để tự động chạy Glue Job Kiểm tra dữ liệu trong S3 bucket fashion-clean-zone Tạo Glue Crawler để tự động tạo Glue Data Catalog cho dữ liệu trong S3 bucket fashion-clean-zone",
    "description": "Các bước thực hiện Trong phần này chúng ta sẽ sử dụng Glue ETL để thực hiện chuyển đổi dữ liệu từ S3 bucket fashion-landing-zone sang S3 bucket fashion-clean-zone. Với mục đích sẵn sàng cho việc phân tích dữ liệu cũng như lưu trữ dữ liệu.\nPhần này sẽ bao gồm các bước sau:\nTạo Glue Job để chuyển đổi dữ liệu từ S3 bucket fashion-landing-zone sang S3 bucket fashion-clean-zone Tạo Glue Trigger để tự động chạy Glue Job Kiểm tra dữ liệu trong S3 bucket fashion-clean-zone Tạo Glue Crawler để tự động tạo Glue Data Catalog cho dữ liệu trong S3 bucket fashion-clean-zone",
    "tags": [],
    "title": "6. Chuyển đổi dữ liệu",
    "uri": "/vi/transform/index.html"
  },
  {
    "breadcrumb": "AWS Fashion Pipeline \u003e  2. Các bước chuẩn bị",
    "content": "Tạo Kinesis Firehose Truy cập Amazon Management Console Tìm kiếm dịch vụ Firehose Chọn Amazon Data Firehose từ kết quả tìm kiếm Trong phần giao diện Kinesis Firehose chọn Create Firehose stream Trong giao diện Create Firehose stream Chọn Source là Amazon Kinesis Data Stream Chọn Destination là Amazon S3 Chọn Name là fashion-ds-firehose Trong source settings chọn browse và chọn Kinesis Data Stream đã tạo ở bước trước. Trong phần Destination setting chọn Browse và chọn bucket fashion-landing-zone. Phần New line delimiter: Chọn Enabled Tại S3 bucket prefix nhập clickstreams/ Dưới buffer hints, compression, file extension and encryption: Để mặc định bufer size là 5MB Để mặc định bufer interval là 300s Chọn Create Firehose stream",
    "description": "Tạo Kinesis Firehose Truy cập Amazon Management Console Tìm kiếm dịch vụ Firehose Chọn Amazon Data Firehose từ kết quả tìm kiếm Trong phần giao diện Kinesis Firehose chọn Create Firehose stream Trong giao diện Create Firehose stream Chọn Source là Amazon Kinesis Data Stream Chọn Destination là Amazon S3 Chọn Name là fashion-ds-firehose Trong source settings chọn browse và chọn Kinesis Data Stream đã tạo ở bước trước. Trong phần Destination setting chọn Browse và chọn bucket fashion-landing-zone. Phần New line delimiter: Chọn Enabled Tại S3 bucket prefix nhập clickstreams/ Dưới buffer hints, compression, file extension and encryption: Để mặc định bufer size là 5MB Để mặc định bufer interval là 300s Chọn Create Firehose stream",
    "tags": [],
    "title": "2.7 Tạo Kinesis Firehose",
    "uri": "/vi/preparation/setup-firehose/index.html"
  },
  {
    "breadcrumb": "AWS Fashion Pipeline",
    "content": "Các bước thực hiện Tạo DynamoDB Table để hiện gợi ý sản phẩm cho người dùng",
    "description": "Các bước thực hiện Tạo DynamoDB Table để hiện gợi ý sản phẩm cho người dùng",
    "tags": [],
    "title": "7. Tạo hệ thống gợi ý sản phẩm",
    "uri": "/vi/recommended/index.html"
  },
  {
    "breadcrumb": "AWS Fashion Pipeline",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Thể loại",
    "uri": "/vi/categories/index.html"
  },
  {
    "breadcrumb": "AWS Fashion Pipeline",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Thẻ",
    "uri": "/vi/tags/index.html"
  }
]
