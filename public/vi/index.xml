<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>AWS Fashion Pipeline</title>
    <link>http://localhost:1313/vi/index.html</link>
    <description>Trong workshop lần này, chúng ta sẽ cùng xây dựng một Data Lake trên AWS cho cửa hàng bán quần áo online với hệ thống như sau:</description>
    <generator>Hugo</generator>
    <language>vi</language>
    <atom:link href="http://localhost:1313/vi/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>1. Giới thiệu</title>
      <link>http://localhost:1313/vi/overview/index.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/vi/overview/index.html</guid>
      <description>A Data Pipeline for Fashion Store Dữ liệu Workshop này bao gồm những dữ liệu sau:&#xA;Các sản phẩm quần áo được lấy từ Kaggle Dữ liệu người dùng, mua hàng, clickstream tự tạo bằng Faker Python Bối cảnh Giả sử bạn là Data Engineer tại một cửa hàng bán quần áo online, sếp bạn muốn biết sự quan tâm của khách hàng vào những sản phẩm nào cũng như là phân tích xem xu hướng thời trang hiện tại như thế nào.</description>
    </item>
    <item>
      <title>2. Các bước chuẩn bị</title>
      <link>http://localhost:1313/vi/preparation/index.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/vi/preparation/index.html</guid>
      <description>Tổng quan Đầu tiên là tạo một môi trường ảo, coi như đây là môi trường ứng dụng của cửa hàng. EC2 Instance sẽ là máy chủ giả, nơi tạo ra dữ liệu cho lab. RDS Instance sẽ là cơ sở dữ liệu, dành cho các giao dịch… S3 Bucket sẽ là Data Lake, bao gồm các bucket sau: fashion-landing-zone: Nơi lưu trữ dữ liệu thô từ các nguồn khác nhau. fashion-clean-zone: Nơi lưu trữ dữ liệu đã được xử lý và chuẩn bị cho việc phân tích. fashion-logic-zone: Nơi lưu trữ các script Lambda và mã nguồn cho các quy trình ETL (Extract, Transform, Load). Kinesis Stream sẽ là luồng lưu trữ các sự kiện tương tác của khách hàng Các bước triển khai Trước khi bắt đầu, bạn cần chuẩn bị các phần code cho các bước triển khai. Link github của dự án là: aws-fashion-data-pipeline</description>
    </item>
    <item>
      <title>3. Tạo môi trường ứng dụng</title>
      <link>http://localhost:1313/vi/generate-data/index.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/vi/generate-data/index.html</guid>
      <description>Tổng quan Trong phần này, ta sẽ tạo môi trường ứng dụng cho Data Pipeline. Môi trường bao gồm: EC2 Instance: Nơi tạo ra dữ liệu giả và khởi tạo các table database. RDS: Database PostgreSQL để lưu trữ dữ liệu giả. Mô hình database bao gồm 4 bảng:&#xA;products, users, orders và order_details Các bước triển khai Quay lại giao diện RDS Chọn Databases từ menu bên trái Bấm vào fashion-db</description>
    </item>
    <item>
      <title>4. Trích xuất dữ liệu từ RDS vào S3</title>
      <link>http://localhost:1313/vi/rds-to-s3/index.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/vi/rds-to-s3/index.html</guid>
      <description>Tổng quan Trong phần này, chúng ta sẽ sử dụng AWS Lambda để trích xuất dữ liệu từ Amazon RDS và lưu vào Amazon S3. Chúng ta sẽ sử dụng một hàm Lambda đơn giản để thực hiện việc này. Hàm Lambda sẽ được kích hoạt sẽ kết nối đến Amazon RDS, thực hiện truy vấn SQL để lấy dữ liệu và sau đó lưu dữ liệu vào Amazon S3 dưới dạng file CSV. Đầu tiên tải zip file Lambda function tại đây, bấm nút download raw file góc bên phải màn hình để tải xuống. Truy cập vào S3 Truy cập vào AWS Console, tìm kiếm và chọn S3 từ menu dịch vụ. Bấm vào bucket fashion-logic-zone mà chúng ta đã tạo ở phần trước. Tại giao diện bucket, bấm Upload 4. Tại giao diện Upload:</description>
    </item>
    <item>
      <title>5. Test dữ liệu</title>
      <link>http://localhost:1313/vi/test-connection/index.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/vi/test-connection/index.html</guid>
      <description>Tạo dữ liệu để test. Vào lại terminal kết nối với EC2 Instances. Chạy lệnh sau đi đến thư mục chứa file tạo dữ liệu: cd /home/ec2-user/project/ec2 Chạy lệnh sau để tạo dữ liệu: python data-generator.py Sau khi chạy một lúc (để khoảng 5 phút), hãy bấm Ctrl + C để dừng lại. Ta có thể thấy dữ liệu output trên terminal như sau. Bạn có thể thấy như sau: Order data insert successfully là dữ liệu order được đẩy vào RDS. Còn lại là dữ liệu tương tác khách hàng được đẩy vào Kinesis stream, với những event như page_view, add_to_cart, product_view.</description>
    </item>
    <item>
      <title>6. Chuyển đổi dữ liệu</title>
      <link>http://localhost:1313/vi/transform/index.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/vi/transform/index.html</guid>
      <description>Các bước thực hiện Trong phần này chúng ta sẽ sử dụng Glue ETL để thực hiện chuyển đổi dữ liệu từ S3 bucket fashion-landing-zone sang S3 bucket fashion-clean-zone. Với mục đích sẵn sàng cho việc phân tích dữ liệu cũng như lưu trữ dữ liệu.&#xA;Phần này sẽ bao gồm các bước sau:&#xA;Tạo Glue Job để chuyển đổi dữ liệu từ S3 bucket fashion-landing-zone sang S3 bucket fashion-clean-zone Tạo Glue Trigger để tự động chạy Glue Job Kiểm tra dữ liệu trong S3 bucket fashion-clean-zone Tạo Glue Crawler để tự động tạo Glue Data Catalog cho dữ liệu trong S3 bucket fashion-clean-zone</description>
    </item>
    <item>
      <title>7. Tạo hệ thống gợi ý sản phẩm</title>
      <link>http://localhost:1313/vi/recommended/index.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/vi/recommended/index.html</guid>
      <description>Giới thiệu Trong phần này, chúng ta sẽ xây dựng một hệ thống gợi ý sản phẩm dựa trên dữ liệu đã được chuyển đổi và lưu trữ trong S3 bucket fashion-clean-zone. Hệ thống này sẽ sử dụng Amazon DynamoDB để lưu trữ các gợi ý sản phẩm cho người dùng.&#xA;Hệ thống này hoạt động dựa trên phân tích dữ liệu người dùng theo gần thời gian thực, bằng cách mỗi khi nguười dùng thao tác với sản phẩm như xem, thêm vaào giỏ hàng thì hệ thống sẽ ghi nhận và đưa ra những gợi ý cho các sản phẩm liên quan</description>
    </item>
    <item>
      <title>8. Lên lịch cho các nhiệm vụ định kỳ</title>
      <link>http://localhost:1313/vi/event-bridge/index.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/vi/event-bridge/index.html</guid>
      <description>Giới thiệu Trong phần này, chúng ta sẽ thiết lập các nhiệm vụ định kỳ để tự động hóa quá trình cập nhật dữ liệu và gợi ý sản phẩm trong hệ thống. Chúng ta sẽ sử dụng Amazon EventBridge để lên lịch cho các nhiệm vụ này.&#xA;Các bước thực hiện Trong AWS Management Console, tìm kiếm và chọn dịch vụ EventBridge. Chọn EventBridge Schedule rồi chọn Create rules ở thanh bên trái.</description>
    </item>
    <item>
      <title>9. Dọn dẹp các tài nguyên</title>
      <link>http://localhost:1313/vi/clean/index.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/vi/clean/index.html</guid>
      <description>S3 Vào dịch vụ S3, chọn các bucket đã tạo trong quá trình thực hiện dự án. Chọn Empty để xóa tất cả các đối tượng trong bucket. Sau khi xóa, chọn Delete để xóa bucket. DynamoDB Vào dịch vụ DynamoDB, chọn Tables từ menu bên trái. Chọn bảng fashion-rcm-table đã tạo trong quá trình thực hiện dự án. Chọn Delete để xóa bảng. Xác nhận việc xóa bảng bằng cách nhập confirm và bấm Delete. Glue Vào dịch vụ Glue, chọn ETL jobs từ menu bên trái.</description>
    </item>
  </channel>
</rss>