<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>AWS Fashion Pipeline</title>
    <link>http://localhost:1313/vi/index.html</link>
    <description>Trong workshop lần này, chúng ta sẽ cùng xây dựng một Data Lake trên AWS cho cửa hàng bán quần áo online với hệ thống như sau:</description>
    <generator>Hugo</generator>
    <language>vi</language>
    <atom:link href="http://localhost:1313/vi/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>1. Giới thiệu</title>
      <link>http://localhost:1313/vi/overview/index.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/vi/overview/index.html</guid>
      <description>A Data Pipeline for Fashion Store Dữ liệu Workshop này bao gồm những dữ liệu sau:&#xA;Các sản phẩm quần áo được lấy từ Kaggle Dữ liệu người dùng, mua hàng, clickstream tự tạo bằng Faker Python Bối cảnh Giả sử bạn là Data Engineer tại một cửa hàng bán quần áo online, sếp bạn muốn biết sự quan tâm của khách hàng vào những sản phẩm nào cũng như là phân tích xem xu hướng thời trang hiện tại như thế nào.</description>
    </item>
    <item>
      <title>2. Các bước chuẩn bị</title>
      <link>http://localhost:1313/vi/preparation/index.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/vi/preparation/index.html</guid>
      <description>Tổng quan Đầu tiên là tạo một môi trường ảo, coi như đây là môi trường ứng dụng của cửa hàng. EC2 Instance sẽ là máy chủ giả, nơi tạo ra dữ liệu cho lab. RDS Instance sẽ là cơ sở dữ liệu, dành cho các giao dịch… S3 Bucket sẽ là Data Lake, bao gồm các bucket sau: fashion-landing-zone: Nơi lưu trữ dữ liệu thô từ các nguồn khác nhau. fashion-clean-zone: Nơi lưu trữ dữ liệu đã được xử lý và chuẩn bị cho việc phân tích. fashion-logic-zone: Nơi lưu trữ các script Lambda và mã nguồn cho các quy trình ETL (Extract, Transform, Load). Kinesis Stream sẽ là luồng lưu trữ các sự kiện tương tác của khách hàng Các bước triển khai Đảm bảo để Region là Singapore (ap-southeast-1) trước khi thực hiện các bước sau:</description>
    </item>
    <item>
      <title>3. Tạo môi trường ứng dụng</title>
      <link>http://localhost:1313/vi/generate-data/index.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/vi/generate-data/index.html</guid>
      <description>Tổng quan Trong phần này, ta sẽ tạo môi trường ứng dụng cho Data Pipeline. Môi trường bao gồm: EC2 Instance: Nơi tạo ra dữ liệu giả và khởi tạo các table database. RDS: Database PostgreSQL để lưu trữ dữ liệu giả. Mô hình database bao gồm 4 bảng:&#xA;products, users, orders và order_details Các bước triển khai Quay lại giao diện RDS Chọn Databases từ menu bên trái Bấm vào fashion-db</description>
    </item>
    <item>
      <title>4. Trích xuất dữ liệu từ RDS vào S3</title>
      <link>http://localhost:1313/vi/rds-to-s3/index.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/vi/rds-to-s3/index.html</guid>
      <description>Tổng quan Trong phần này, chúng ta sẽ sử dụng AWS Lambda để trích xuất dữ liệu từ Amazon RDS và lưu vào Amazon S3. Chúng ta sẽ sử dụng một hàm Lambda đơn giản để thực hiện việc này. Hàm Lambda sẽ được kích hoạt sẽ kết nối đến Amazon RDS, thực hiện truy vấn SQL để lấy dữ liệu và sau đó lưu dữ liệu vào Amazon S3 dưới dạng file CSV. Đầu tiên tải zip file Lambda function tại đây, bấm nút download raw file góc bên phải màn hình để tải xuống. Truy cập vào S3 Truy cập vào AWS Console, tìm kiếm và chọn S3 từ menu dịch vụ. Bấm vào bucket fashion-logic-zone mà chúng ta đã tạo ở phần trước. Tại giao diện bucket, bấm Upload 4. Tại giao diện Upload:</description>
    </item>
    <item>
      <title>5. Test dữ liệu</title>
      <link>http://localhost:1313/vi/test-connection/index.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/vi/test-connection/index.html</guid>
      <description>Tạo dữ liệu để test. Vào lại terminal kết nối với EC2 Instances. Chạy lệnh sau đi đến thư mục chứa file tạo dữ liệu: cd /home/ec2-user/project/ec2 Chạy lệnh sau để tạo dữ liệu: python data-generator.py Sau khi chạy một lúc (để khoảng 5 phút), hãy bấm Ctrl + C để dừng lại. Ta có thể thấy dữ liệu output trên terminal như sau. Bạn có thể thấy như sau: Order data insert successfully là dữ liệu order được đẩy vào RDS. Còn lại là dữ liệu tương tác khách hàng được đẩy vào Kinesis stream, với những event như page_view, add_to_cart, product_view.</description>
    </item>
    <item>
      <title>6. Chuyển đổi dữ liệu</title>
      <link>http://localhost:1313/vi/transform/index.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/vi/transform/index.html</guid>
      <description>Các bước thực hiện Trong phần này chúng ta sẽ sử dụng Glue ETL để thực hiện chuyển đổi dữ liệu từ S3 bucket fashion-landing-zone sang S3 bucket fashion-clean-zone. Với mục đích sẵn sàng cho việc phân tích dữ liệu cũng như lưu trữ dữ liệu.&#xA;Phần này sẽ bao gồm các bước sau:&#xA;Tạo Glue Job để chuyển đổi dữ liệu từ S3 bucket fashion-landing-zone sang S3 bucket fashion-clean-zone Tạo Glue Trigger để tự động chạy Glue Job Kiểm tra dữ liệu trong S3 bucket fashion-clean-zone Tạo Glue Crawler để tự động tạo Glue Data Catalog cho dữ liệu trong S3 bucket fashion-clean-zone</description>
    </item>
    <item>
      <title>7. Tạo hệ thống gợi ý sản phẩm</title>
      <link>http://localhost:1313/vi/recommended/index.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/vi/recommended/index.html</guid>
      <description>Các bước thực hiện Tạo DynamoDB Table để hiện gợi ý sản phẩm cho người dùng</description>
    </item>
  </channel>
</rss>