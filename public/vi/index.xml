<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>AWS Fashion Pipeline</title>
    <link>http://localhost:1313/vi/index.html</link>
    <description>Trong workshop lần này, chúng ta sẽ cùng xây dựng một Data Lake trên AWS cho cửa hàng bán quần áo online với hệ thống như sau:&#xA;module:&#xD;mounts:&#xD;- source: assets&#xD;target: assets&#xD;- source: static&#xD;target: assets</description>
    <generator>Hugo</generator>
    <language>vi</language>
    <atom:link href="http://localhost:1313/vi/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>1. Giới thiệu</title>
      <link>http://localhost:1313/vi/overview/index.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/vi/overview/index.html</guid>
      <description>A Data Pipeline for Fashion Store Dữ liệu Workshop này bao gồm những dữ liệu sau:&#xA;Các sản phẩm quần áo được lấy từ Kaggle Dữ liệu người dùng, mua hàng, clickstream tự tạo bằng Faker Python Bối cảnh Giả sử bạn là Data Engineer tại một cửa hàng bán quần áo online, sếp bạn muốn biết sự quan tâm của khách hàng vào những sản phẩm nào cũng như là phân tích xem xu hướng thời trang hiện tại như thế nào.</description>
    </item>
    <item>
      <title>2. Các bước chuẩn bị</title>
      <link>http://localhost:1313/vi/preparation/index.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/vi/preparation/index.html</guid>
      <description>Tổng quan Đầu tiên là tạo một môi trường ảo, coi như đây là môi trường ứng dụng của cửa hàng. EC2 Instance sẽ là máy chủ giả, nơi tạo ra dữ liệu cho lab. RDS Instance sẽ là cơ sở dữ liệu, dành cho các giao dịch… S3 Bucket sẽ là Data Lake, bao gồm các bucket sau: fashion-landing-zone: Nơi lưu trữ dữ liệu thô từ các nguồn khác nhau. fashion-clean-zone: Nơi lưu trữ dữ liệu đã được xử lý và chuẩn bị cho việc phân tích. fashion-logic-zone: Nơi lưu trữ các script Lambda và mã nguồn cho các quy trình ETL (Extract, Transform, Load). Kinesis Stream sẽ là luồng lưu trữ các sự kiện tương tác của khách hàng Các bước triển khai Đảm bảo để Region là Singapore (ap-southeast-1) trước khi thực hiện các bước sau:</description>
    </item>
    <item>
      <title>3. Tạo môi trường ứng dụng</title>
      <link>http://localhost:1313/vi/generate-data/index.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/vi/generate-data/index.html</guid>
      <description>Tổng quan Trong phần này, ta sẽ tạo môi trường ứng dụng cho Data Pipeline. Môi trường bao gồm: EC2 Instance: Nơi tạo ra dữ liệu giả và khởi tạo các table database. RDS: Database PostgreSQL để lưu trữ dữ liệu giả. Mô hình database bao gồm 4 bảng:&#xA;products, users, orders và order_details module:&#xD;mounts:&#xD;- source: assets&#xD;target: assets&#xD;- source: static&#xD;target: assets&#xA;Các bước triển khai Quay lại giao diện RDS Chọn Databases từ menu bên trái Bấm vào fashion-db module:&#xD;mounts:&#xD;- source: assets&#xD;target: assets&#xD;- source: static&#xD;target: assets</description>
    </item>
    <item>
      <title>4. Trích xuất dữ liệu từ RDS vào S3</title>
      <link>http://localhost:1313/vi/rds-to-s3/index.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/vi/rds-to-s3/index.html</guid>
      <description>Tổng quan Trong phần này, chúng ta sẽ sử dụng AWS Lambda để trích xuất dữ liệu từ Amazon RDS và lưu vào Amazon S3. Chúng ta sẽ sử dụng một hàm Lambda đơn giản để thực hiện việc này. Hàm Lambda sẽ được kích hoạt sẽ kết nối đến Amazon RDS, thực hiện truy vấn SQL để lấy dữ liệu và sau đó lưu dữ liệu vào Amazon S3 dưới dạng file CSV. Đầu tiên tải zip file Lambda function module:&#xD;mounts:&#xD;- source: assets&#xD;target: assets&#xD;- source: static&#xD;target: assets, bấm nút download raw file góc bên phải màn hình để tải xuống. Đây là code của hàm Lambda, chúng ta sẽ không đi sâu vào chi tiết code trong phần này. Chúng ta chỉ cần biết rằng hàm Lambda này sẽ kết nối đến Amazon RDS, thực hiện truy vấn SQL và lưu dữ liệu vào Amazon S3. import psycopg2 from datetime import datetime, timedelta import boto3 import os import pandas as pd # Variables RDS_HOST = os.getenv(&#34;RDS_HOST&#34;) RDS_PASSWORD = os.getenv(&#34;RDS_PASSWORD&#34;) db_params = { &#39;dbname&#39;: &#39;postgres&#39;, &#39;user&#39;: &#39;postgres&#39;, &#39;password&#39;: RDS_PASSWORD, &#39;host&#39;: RDS_HOST, &#39;port&#39;: 5432 } DATE_EXECUTION = None # S3 RAW_BUCKET = os.getenv(&#34;S3_BUCKET&#34;) s3 = boto3.client(&#39;s3&#39;) def get_table_data(table_name, query): conn = psycopg2.connect(**db_params) cursor = conn.cursor() print(f&#34;Getting table {table_name} data...&#34;) # Execute the query cursor.execute(query) # Fetch all records records = cursor.fetchall() # Get column names col_names = [desc[0] for desc in cursor.description] # Convert to DataFrame df = pd.DataFrame(records, columns=col_names) print(f&#34;Successfully retrieved records from {table_name} table.&#34;) return df def put_csv_to_s3(df, bucket_name, file_name): # Convert DataFrame to CSV csv_data = df.to_csv(index=False) # Upload to S3 s3.put_object(Bucket=bucket_name, Key=file_name, Body=csv_data) print(f&#34;File {file_name} uploaded to S3 bucket {bucket_name}&#34;) def lambda_handler(event, context): try: DATE_EXECUTION = event[&#34;DATE_EXECUTION&#34;] except Exception as e: DATE_EXECUTION = (datetime.now() - timedelta(days=1)).strftime(&#34;%Y-%m-%d&#34;) # Orders Table table_name = &#34;orders&#34; orders_query = f&#34;&#34;&#34; SELECT * FROM orders WHERE order_date &gt;= &#39;{DATE_EXECUTION} 00:00:00&#39; AND order_date &lt;= &#39;{DATE_EXECUTION} 23:59:59&#39; &#34;&#34;&#34; orders_df = get_table_data(table_name, orders_query) orders_file_name = f&#34;orders/orders_{DATE_EXECUTION}.csv&#34; put_csv_to_s3(orders_df, RAW_BUCKET, orders_file_name) # Order Details Table table_name = &#34;order_details&#34; order_details_query = f&#34;&#34;&#34; SELECT * FROM order_details WHERE order_id IN (SELECT id FROM orders WHERE order_date &gt;= &#39;{DATE_EXECUTION} 00:00:00&#39; AND order_date &lt;= &#39;{DATE_EXECUTION} 23:59:59&#39;) &#34;&#34;&#34; order_details_df = get_table_data(table_name, order_details_query) order_details_file_name = f&#34;order_details/order_details_{DATE_EXECUTION}.csv&#34; put_csv_to_s3(order_details_df, RAW_BUCKET, order_details_file_name) # Users Table table_name = &#34;users&#34; users_query = f&#34;&#34;&#34; SELECT * FROM users &#34;&#34;&#34; users_df = get_table_data(table_name, users_query) users_file_name = f&#34;users/users_{DATE_EXECUTION}.csv&#34; put_csv_to_s3(users_df, RAW_BUCKET, users_file_name) # Products Table table_name = &#34;products&#34; products_query = f&#34;&#34;&#34; SELECT * FROM products &#34;&#34;&#34; products_df = get_table_data(table_name, products_query) products_file_name = f&#34;products/products_{DATE_EXECUTION}.csv&#34; put_csv_to_s3(products_df, RAW_BUCKET, products_file_name) print(&#34;All data has been successfully uploaded to S3.&#34;) Truy cập vào S3 Truy cập vào AWS Console, tìm kiếm và chọn S3 từ menu dịch vụ. Bấm vào bucket fashion-logic-zone mà chúng ta đã tạo ở phần trước. Tại giao diện bucket, bấm Upload module:&#xD;mounts:&#xD;- source: assets&#xD;target: assets&#xD;- source: static&#xD;target: assets 4. Tại giao diện Upload:</description>
    </item>
    <item>
      <title>5. Test dữ liệu</title>
      <link>http://localhost:1313/vi/test-connection/index.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/vi/test-connection/index.html</guid>
      <description>Tạo dữ liệu để test. Vào lại terminal kết nối với EC2 Instances. Chạy lệnh sau đi đến thư mục chứa file tạo dữ liệu: cd /home/ec2-user/project/ec2 Chạy lệnh sau để tạo dữ liệu: python data-generator.py Sau khi chạy một lúc, hãy bấm Ctrl + C để dừng lại. Ta có thể thấy dữ liệu output trên terminal như sau. module:&#xD;mounts:&#xD;- source: assets&#xD;target: assets&#xD;- source: static&#xD;target: assets</description>
    </item>
  </channel>
</rss>