var relearn_searchindex = [
  {
    "breadcrumb": "AWS Fashion Pipeline",
    "content": "A Data Pipeline for Fashion Store Dữ liệu Workshop này bao gồm những dữ liệu sau:\nCác sản phẩm quần áo được lấy từ Kaggle Dữ liệu người dùng, mua hàng, clickstream tự tạo bằng Faker Python Bối cảnh Giả sử bạn là Data Engineer tại một cửa hàng bán quần áo online, sếp bạn muốn biết sự quan tâm của khách hàng vào những sản phẩm nào cũng như là phân tích xem xu hướng thời trang hiện tại như thế nào.\nSếp của bạn có những yêu cầu sau:\nKhi người dùng bấm vào hoặc thêm vào giỏ hàng một sản phẩm, trực tiếp được chuyển vào kho lưu trữ, đồng thời chuyển đổi và làm giàu để ML có thể dự đoán xu hướng của thời trang hiện tại và đưa ra các gợi ý sản phẩm cho người dùng real-time. Sau đó dữ liệu gợi ý cho từng User được lưu vào nơi để có thể trực tiếp gợi ý cho người dùng ở trên webiste. Yêu cầu thứ 2 là, sau khi người dùng mua hàng, thì kết thúc một ngày, dữ liệu cần được tổng hợp, làm sạch và chuyển vào kho lưu trữ để sẵn sàng cho mục đích phân tích để tìm ra hướng cho chiến lược marketing tiếp theo. Và ngay sau đó bạn nảy ra ý tưởng triển khai trên Amazon Web Service như sau.\nÝ tưởng Bạn nghĩ ra một hệ thống kết hợp giữa Batch Processing và Real-Time Processing như sau:\nTriển khai hệ thống Database trên Amazon RDS, nơi xử lý các giao dịch mua hàng của người dùng,…\nTriển khai hệ thống Data Lake bằng Amazon S3 trên AWS để lưu trữ dữ liệu tổng hợp của cửa hàng.\nXây dựng Glue Data Catalog để có thể trực tiếp phân tích dữ liệu từ Data Lake bằng Athena, hoặc Redshift Spectrum\nĐối với luồng dữ liệu tương tác khách hàng (Clickstream), ta sẽ tạo một Kinesis Data Stream sau đó có 2 Consumer:\nDữ liệu trực tuyến đi qua Lambda, và mỗi khi người dùng tương tác sẽ kích hoạt Lambda Function gợi ý sản phẩm đẩy vào DynamoDB. Consumer còn lại đi vào Kinesis Firehose mỗi 5 phút, để đẩy vào Data Lake, nơi Data Analysis phân tích dữ liệu và đưa ra xu hướng thời trang, cũng như là doanh số,… Lên lịch cho những công việc với Amazon MWAA\nDữ liệu clickstream đi vào Lambda mỗi 5 phút. Sau một ngày dữ liệu từ RDS đi vào S3 vào 00:01 ngày hôm sau. Sau lúc bạn ghi ra những ý tưởng, đầu bạn nảy ra một hệ thống trên AWS có kiến trúc như sau\nKiến trúc hệ thống Lưu ý: Trong phần này, chúng ta sẽ không triển khai dịch vụ QuickSight và Redshift Spectrum, mà chỉ tập trung vào các dịch vụ chính để xây dựng hệ thống gợi ý sản phẩm.",
    "description": "A Data Pipeline for Fashion Store Dữ liệu Workshop này bao gồm những dữ liệu sau:\nCác sản phẩm quần áo được lấy từ Kaggle Dữ liệu người dùng, mua hàng, clickstream tự tạo bằng Faker Python Bối cảnh Giả sử bạn là Data Engineer tại một cửa hàng bán quần áo online, sếp bạn muốn biết sự quan tâm của khách hàng vào những sản phẩm nào cũng như là phân tích xem xu hướng thời trang hiện tại như thế nào.",
    "tags": [],
    "title": "1. Giới thiệu",
    "uri": "/overview/index.html"
  },
  {
    "breadcrumb": "AWS Fashion Pipeline \u003e  2. Các bước chuẩn bị",
    "content": "Tổng quan Trong phần này chúng ta sẽ tạo các IAM Role cần thiết cho các dịch vụ AWS mà chúng ta sẽ sử dụng trong dự án này. Các role này sẽ cho phép các dịch vụ AWS truy cập vào các tài nguyên khác nhau trong tài khoản AWS.\nCác role này sẽ bao gồm:\nRole cho Lambda truy cập vào S3 Role cho EC2 truy cập vào Kinesis Role cho Lambda truy cập vào Kinesis và DynamoDB Role cho Glue truy cập vào S3 Tạo role cho Lambda truy cập vào S3 Truy cập vào AWS Console, chọn IAM từ menu dịch vụ. Chọn Roles từ menu bên trái, bấm vào Create role. Tại giao diện Select trusted entity, chọn AWS service, sau đó chọn Lambda từ danh sách dịch vụ. Bấm Next. Tại phần Add permissions\nTìm kiếm và chọn AmazonS3FullAccess từ danh sách chính sách. Tìm kiếm và chọn AWSLambdaENIManagementAccess từ danh sách chính sách. Bấm Next. Tại phần Name, review, and create\nNhập tên cho role là lambda-s3-full-access. Phần description nhập Allow Lambda access S3 with Full Access. Bấm Create role. Tạo role cho EC2 truy cập vào Kinesis Truy cập vào AWS Console, chọn IAM từ menu dịch vụ. Chọn Roles từ menu bên trái, bấm vào Create role. Tại giao diện Select trusted entity, chọn AWS service, sau đó chọn EC2 từ danh sách dịch vụ. Bấm Next. Tại phần Add permissions Tìm kiếm và chọn AmazonKinesisFullAccess từ danh sách chính sách. Bấm Next. Tại phần Name, review, and create Nhập tên cho role là ec2-kinesis-role. Phần description nhập Allow EC2 access Kinesis with Full Access. Bấm Create role. Tạo role cho Lambda truy cập vào Kinesis và DynamoDB Tương tự như cách tạo role cho các bước trên, tạo role với use case là Lambda. Tại phần Add permissions Tìm kiếm và chọn AmazonKinesisFullAccess từ danh sách chính sách. Tìm kiếm và chọn AmazonDynamoDBFullAccess từ danh sách chính sách. Tìm kiếm và chọn AmazonAthenaFullAccess từ danh sách chính sách Tìm kiếm và chọn AWSLambdaBasicExecutionRole từ danh sách chính sách. Tim kiếm và chọn AmazonS3FullAccess từ danh sách chính sách. Bấm Next. Tại phần Name, review, and create Nhập tên cho role là lambda-kinesis-dynamodb-role. Phần description nhập Allow Lambda access Kinesis and DynamoDB and S3 with Full Access. Bấm Create role. Tạo role cho Glue truy cập vào S3 Tương tự như cách tạo role cho các bước trên, tạo role với use case là Glue. Tại phần Add permissions Tìm kiếm và chọn AmazonS3FullAccess từ danh sách chính sách. Tìm kiếm và chọn AWSGlueServiceRole từ danh sách chính sách. Bấm Next. Tại phần Name, review, and create Nhập tên cho role là AWS-Glue-S3-Full-Access . Phần description nhập Allow Glue access to S3 with Full Access . Bấm Create role.",
    "description": "Tổng quan Trong phần này chúng ta sẽ tạo các IAM Role cần thiết cho các dịch vụ AWS mà chúng ta sẽ sử dụng trong dự án này. Các role này sẽ cho phép các dịch vụ AWS truy cập vào các tài nguyên khác nhau trong tài khoản AWS.\nCác role này sẽ bao gồm:\nRole cho Lambda truy cập vào S3 Role cho EC2 truy cập vào Kinesis Role cho Lambda truy cập vào Kinesis và DynamoDB Role cho Glue truy cập vào S3 Tạo role cho Lambda truy cập vào S3 Truy cập vào AWS Console, chọn IAM từ menu dịch vụ.",
    "tags": [],
    "title": "2.1 Tạo các Role cần thiết",
    "uri": "/preparation/setup-role/index.html"
  },
  {
    "breadcrumb": "AWS Fashion Pipeline \u003e  5. Test dữ liệu",
    "content": "Kiểm tra dữ liệu trong RDS. Sử dụng lệnh truy cập vào PostgreSQL ở bước trước. psql -U postgres -h \u003cYOUR_POSTGRESQL_DNS\u003e -p 5432 -d postgres Nếu bạn thấy thông báo như sau thì bạn đã kết nối thành công tới database PostgreSQL. Chạy lệnh sau để kiểm tra dữ liệu trong bảng orders:\nSELECT * FROM orders; Chạy lệnh sau để kiểm tra dữ liệu trong bảng order_details: SELECT * FROM order_details; Dưới đây là hình ảnh bảng orders ví dụ, có thể dữ liệu sẽ khác nhau do random. Dưới đây là hình ảnh bảng order_details ví dụ, có thể dữ liệu sẽ khác nhau do random. Gõ \\q để thoát khỏi database PostgreSQL. và trở về terminal. Vậy là bạn đã hoàn tất việc kiểm tra dữ liệu trong RDS. Bạn có thể sử dụng các câu lệnh SQL khác để kiểm tra dữ liệu trong các bảng khác như users, products hoặc thực hiện các truy vấn phức tạp hơn.",
    "description": "Kiểm tra dữ liệu trong RDS. Sử dụng lệnh truy cập vào PostgreSQL ở bước trước. psql -U postgres -h \u003cYOUR_POSTGRESQL_DNS\u003e -p 5432 -d postgres Nếu bạn thấy thông báo như sau thì bạn đã kết nối thành công tới database PostgreSQL. Chạy lệnh sau để kiểm tra dữ liệu trong bảng orders:\nSELECT * FROM orders; Chạy lệnh sau để kiểm tra dữ liệu trong bảng order_details: SELECT * FROM order_details; Dưới đây là hình ảnh bảng orders ví dụ, có thể dữ liệu sẽ khác nhau do random.",
    "tags": [],
    "title": "5.1 Kiểm tra dữ liệu trong RDS",
    "uri": "/test-connection/test-rds/index.html"
  },
  {
    "breadcrumb": "AWS Fashion Pipeline \u003e  6. Chuyển đổi dữ liệu",
    "content": "Tổng quan Trong phần này chúng ta sẽ tạo Glue Job để chuyển đổi dữ liệu từ S3 bucket fashion-landing-zone sang S3 bucket fashion-clean-zone.\nTạo Glue Job Đăng nhập vào AWS Management Console và tìm kiếm dịch vụ AWS Glue. Tại giao diện AWS Glue, chọn “Go to ETL jobs” từ menu bên trái Tại giao diện ETL jobs, chọn Create job from a blank graph hoặc Visual ETL để tạo Glue Job mới Tại giao diện tạo Glue Job mới, bấm vào Script sau đó bấm vào Edit Script chọn Confirm Sao chép mã từ đường dẫn sau và dán vào ô Script: Glue Job Script Sau đó tại dòng thứ 13, điền tên S3 bucket của bạn vào biến LANDING_BUCKET Tại dòng thứ 14, điền tên S3 bucket của bạn vào biến CLEAN_BUCKET Sau đó bấm vào Job details. Tại giao diện Job details, điền các thông tin sau: Name: fashion-datalake-etl IAM role: Chọn IAM Role “AWS-Glue-S3-Full-Access” mà bạn đã tạo ở phần trước. Kéo xuống dưới cùng, bấm vào Advanced properties Tại Script path: Nhập đường dẫn như sau s3://\u003cYOUR-BUCKET-LOGIC-ZONE\u003e/fashion-datalake-etl/script/ Tại Spark UI logs path: Nhập đường dẫn như sau s3://\u003cYOUR-BUCKET-LOGIC-ZONE\u003e/fashion-datalake-etl/sparkHistoryLogs/ Tại Temporary path: Nhập đường dẫn như sau s3://\u003cYOUR-BUCKET-LOGIC-ZONE\u003e/fashion-datalake-etl/temporary/ Bấm Save để lưu lại Glue Job. Lên lịch chạy Glue Job tự động sau một ngày: Tại giao diện Glue Job, chọn Schedules từ menu bên trái Bấm Create schedule Tại giao diện tạo lịch chạy Glue Job vào 0:00 hằng ngày, điền các thông tin sau: Name: fashion-datalake-etl-schedule Frequency: Chọn Daily Start hour: Nhập 17 vì lịch chạy theo giờ UTC, nên cần -7 giờ để chạy vào 0:00 giờ theo giờ Việt Nam. Minute of the hour: Nhập 0 Chọn Create schedule để tạo lịch chạy Glue Job Chạy Glue Job Tại giao diện Glue Job, chọn Run, sau đó chọn Runs ở menu bên trái để kiểm tra trạng thái chạy Glue Job. Đợi một chút để Glue Job chạy xong, sau đó kiểm tra trạng thái Glue Job đã chạy thành công hay chưa. Nếu trạng thái là Succeeded thì Glue Job đã chạy thành công. Kiểm tra dữ liệu trong S3 bucket fashion-clean-zone đã có dữ liệu hay chưa. Nếu có dữ liệu thì Glue Job đã chạy thành công.\nKiểm tra output của Glue Job ở trong CloudWatch Logs\nVào dịch vụ CloudWatch, chọn Log groups Sau đó tìm kiếm log group có tên là /aws-glue/jobs/output và bấm vào Tại đây bạn sẽ thấy các log của Glue Job mà bạn đã chạy.",
    "description": "Tổng quan Trong phần này chúng ta sẽ tạo Glue Job để chuyển đổi dữ liệu từ S3 bucket fashion-landing-zone sang S3 bucket fashion-clean-zone.\nTạo Glue Job Đăng nhập vào AWS Management Console và tìm kiếm dịch vụ AWS Glue. Tại giao diện AWS Glue, chọn “Go to ETL jobs” từ menu bên trái Tại giao diện ETL jobs, chọn Create job from a blank graph hoặc Visual ETL để tạo Glue Job mới",
    "tags": [],
    "title": "6.1 Tạo Glue Job",
    "uri": "/transform/transform/index.html"
  },
  {
    "breadcrumb": "AWS Fashion Pipeline \u003e  7. Tạo hệ thống gợi ý sản phẩm",
    "content": "Các bước thực hiện Truy cập vào AWS Management Console và tìm kiếm dịch vụ DynamoDB. Tại giao diện DynamoDB, chọn “Create table”. Tại giao diện tạo bảng, điền các thông tin sau: Table name: fashion-rcm-table Partition key: user_id Chọn Number Bấm Create table để tạo bảng.",
    "description": "Các bước thực hiện Truy cập vào AWS Management Console và tìm kiếm dịch vụ DynamoDB. Tại giao diện DynamoDB, chọn “Create table”. Tại giao diện tạo bảng, điền các thông tin sau: Table name: fashion-rcm-table Partition key: user_id Chọn Number Bấm Create table để tạo bảng.",
    "tags": [],
    "title": "7.1 Tạo DynamoDB Table",
    "uri": "/recommended/1-create-dynamodb-table/index.html"
  },
  {
    "breadcrumb": "AWS Fashion Pipeline",
    "content": "Tổng quan Đầu tiên là tạo một môi trường ảo, coi như đây là môi trường ứng dụng của cửa hàng. EC2 Instance sẽ là máy chủ giả, nơi tạo ra dữ liệu cho lab. RDS Instance sẽ là cơ sở dữ liệu, dành cho các giao dịch… S3 Bucket sẽ là Data Lake, bao gồm các bucket sau: fashion-landing-zone: Nơi lưu trữ dữ liệu thô từ các nguồn khác nhau. fashion-clean-zone: Nơi lưu trữ dữ liệu đã được xử lý và chuẩn bị cho việc phân tích. fashion-logic-zone: Nơi lưu trữ các script Lambda và mã nguồn cho các quy trình ETL (Extract, Transform, Load). Kinesis Stream sẽ là luồng lưu trữ các sự kiện tương tác của khách hàng Các bước triển khai Trước khi bắt đầu, bạn cần chuẩn bị các phần code cho các bước triển khai. Link github của dự án là: aws-fashion-data-pipeline\nClone repository về máy tính của bạn: git clone https://github.com/ltdungg/aws-fashion-data-pipeline cd aws-fashion-data-pipeline Đảm bảo để Region là Singapore (ap-southeast-1) trước khi thực hiện các bước sau:\n2.1 Tạo các Role cần thiết\n2.2 Tạo VPC, Subnet, …\n2.3 Tạo EC2 Instance\n2.4 Tạo RDS Instance\n2.5 Tạo S3 Bucket\n2.6 Tạo Kinesis Stream",
    "description": "Tổng quan Đầu tiên là tạo một môi trường ảo, coi như đây là môi trường ứng dụng của cửa hàng. EC2 Instance sẽ là máy chủ giả, nơi tạo ra dữ liệu cho lab. RDS Instance sẽ là cơ sở dữ liệu, dành cho các giao dịch… S3 Bucket sẽ là Data Lake, bao gồm các bucket sau: fashion-landing-zone: Nơi lưu trữ dữ liệu thô từ các nguồn khác nhau. fashion-clean-zone: Nơi lưu trữ dữ liệu đã được xử lý và chuẩn bị cho việc phân tích. fashion-logic-zone: Nơi lưu trữ các script Lambda và mã nguồn cho các quy trình ETL (Extract, Transform, Load). Kinesis Stream sẽ là luồng lưu trữ các sự kiện tương tác của khách hàng Các bước triển khai Trước khi bắt đầu, bạn cần chuẩn bị các phần code cho các bước triển khai. Link github của dự án là: aws-fashion-data-pipeline",
    "tags": [],
    "title": "2. Các bước chuẩn bị",
    "uri": "/preparation/index.html"
  },
  {
    "breadcrumb": "AWS Fashion Pipeline \u003e  2. Các bước chuẩn bị",
    "content": "Tạo VPC (Virtual Private Cloud) Truy cập Amazon Management Console Tìm kiếm dịch vụ VPC Chọn VPC từ kết quả tìm kiếm Trong phần giao diện VPC chọn Create VPC Trong giao diện Create VPC Chọn VPC and more name tag auto-generation ghi: fashion IPv4 CIDR block: 10.10.0.0/16 Còn lại để mặc định, chọn Create VPC ở phía bên dưới màn hình.",
    "description": "Tạo VPC (Virtual Private Cloud) Truy cập Amazon Management Console Tìm kiếm dịch vụ VPC Chọn VPC từ kết quả tìm kiếm Trong phần giao diện VPC chọn Create VPC Trong giao diện Create VPC Chọn VPC and more name tag auto-generation ghi: fashion IPv4 CIDR block: 10.10.0.0/16 Còn lại để mặc định, chọn Create VPC ở phía bên dưới màn hình.",
    "tags": [],
    "title": "2.2 Tạo VPC, Subnet, ...",
    "uri": "/preparation/setup-env/index.html"
  },
  {
    "breadcrumb": "AWS Fashion Pipeline \u003e  5. Test dữ liệu",
    "content": "Kiểm tra dữ liệu trong Kinesis Data Stream Quay trở lại terminal kết nối với EC2 Instance tại bước 2.3 Tạo EC2 Instance. Chạy lệnh sau để mô tả luồng Kinesis Data Stream: aws kinesis describe-stream --stream-name fashion-ds Chạy lệnh sau để lấy iterator của shard trong luồng Kinesis Data Stream: aws kinesis get-shard-iterator --stream-name fashion-ds --shard-id shardId-000000000000 --shard-iterator-type TRIM_HORIZON Copy ShardIterator trong kết quả trả về của lệnh trên và chạy lệnh sau để lấy dữ liệu trong luồng Kinesis Data Stream: aws kinesis get-records --shard-iterator \u003cYOUR_SHARD_ITERATOR\u003e Trong phần Records:\nPartitionKey là user ID Data là những dữ liệu JSON được encode trong luồng Kiểm tra dữ liệu, copy một Data bên trong Records. Vào trang Base64 Decode để decode dữ liệu vừa copy.\nDecode UTF-8",
    "description": "Kiểm tra dữ liệu trong Kinesis Data Stream Quay trở lại terminal kết nối với EC2 Instance tại bước 2.3 Tạo EC2 Instance. Chạy lệnh sau để mô tả luồng Kinesis Data Stream: aws kinesis describe-stream --stream-name fashion-ds Chạy lệnh sau để lấy iterator của shard trong luồng Kinesis Data Stream: aws kinesis get-shard-iterator --stream-name fashion-ds --shard-id shardId-000000000000 --shard-iterator-type TRIM_HORIZON Copy ShardIterator trong kết quả trả về của lệnh trên và chạy lệnh sau để lấy dữ liệu trong luồng Kinesis Data Stream: aws kinesis get-records --shard-iterator \u003cYOUR_SHARD_ITERATOR\u003e",
    "tags": [],
    "title": "5.2 Kiểm tra dữ liệu trong Kinesis Data Stream",
    "uri": "/test-connection/test-kinesis/index.html"
  },
  {
    "breadcrumb": "AWS Fashion Pipeline \u003e  6. Chuyển đổi dữ liệu",
    "content": "Tạo Glue Data Catalog cho Landing Zone Truy cập Amazon Management Console Tìm kiếm dịch vụ Glue Chọn Glue từ kết quả tìm kiếm Tạo Database cho Glue Data Catalog Trong phần Glue Data Catalog chọn Databases rồi chọn Add database Nhập tên database là fashion-clean-zone Chọn Create Tạo table cho Glue Data Catalog Bấm vào database fashion-clean-zone vừa tạo Chọn Add table rồi chọn Add tables using a crawler Trong phần crawler properties Nhập tên crawler là fashion-clean-zone-crawler Chọn Next Chọn nguồn dữ liệu bấm vào Add a data source Chọn S3 rồi chọn Browse Chọn bucket fashion-clean-zone rồi chọn Add Chọn Next Trong phần IAM role Chọn Create an IAM role Nhập tên role là AWSGlueServiceRole-FashionCrawlerRole Chọn Next Trong phần Set output and scheduling Target database: Chọn database fashion-clean-zone Frequency chọn Daily và nhập 17:00 (UTC là 00:00 giờ Việt Nam) Chọn Next Trong phần Review Chọn Finish Chọn Run Crawler và đợi quá trình crawler hoàn thành. Quá trình này sẽ mất khoảng 1 phút. Kiểm tra kết quả với Athena Truy cập dịch vụ Athena từ AWS Management Console. Bấm vào Launch Query Editor\nTrong giao diện Query Editor, bấm vào Settings bấm Manage Bên dưới phần Location of query result bấm Browse S3 và chọn bucket fashion-logic-zone Chọn Save Quay lại phần Editor Phía bên trái, chọn Data Source là AwsDataCatalog Chọn Database là fashion-clean-zone Sau đó có thể gõ câu lệnh SQL để truy vấn dữ liệu. Ví dụ: SELECT * FROM \"fashion-clean-zone\".\"clickstreams\" limit 10;",
    "description": "Tạo Glue Data Catalog cho Landing Zone Truy cập Amazon Management Console Tìm kiếm dịch vụ Glue Chọn Glue từ kết quả tìm kiếm Tạo Database cho Glue Data Catalog Trong phần Glue Data Catalog chọn Databases rồi chọn Add database Nhập tên database là fashion-clean-zone Chọn Create Tạo table cho Glue Data Catalog Bấm vào database fashion-clean-zone vừa tạo Chọn Add table rồi chọn Add tables using a crawler",
    "tags": [],
    "title": "6.2 Tạo Catalog cho Clean Zone",
    "uri": "/transform/glue-catalog/index.html"
  },
  {
    "breadcrumb": "AWS Fashion Pipeline \u003e  7. Tạo hệ thống gợi ý sản phẩm",
    "content": "Chuẩn bị Trong phần này yêu cầu trên máy tính của bạn đã cài đặt Docker và AWS CLI. Bạn có thể tham khảo hướng dẫn cài đặt Docker tại Docker Installation và AWS CLI tại AWS CLI Installation.\nCác bước thực hiện Truy cập vào AWS Management Console và tìm kiếm dịch vụ Amazon ECR bằng cách gõ ECR. Trong giao diện Amazon ECR, chọn Create từ menu bên trái. Tại giao diện tạo kho chứa (repository), điền các thông tin sau: Repository name: fashion-ecr-repository Sau đó bấm Create để tạo kho chứa. Tại giao diện kho chứa vừa tạo, chọn repository vừa tạo và bấm vào View push commands để xem các lệnh cần thiết để đẩy Docker Image lên kho chứa. Trong đoạn code đã clone từ git, bạn cần vào thư mục lambda/kinesis-to-dynamo-db và mở terminal tại đó\nMở Docker Desktop và thực hiện các lệnh như trên hiển thị trong giao diện Push commands của AWS ECR trên terminal. Lưu ý: Nếu sử dụng Windows để thực hiện build Docker Image, bạn cần thêm lệnh sau vào lệnh docker build để phù hợp với lambda function --platform linux/amd64 và --provenance=false Ví dụ: docker build -t fashion-ecr-repository . --platform linux/amd64 --provenance=false Sau khi thực hiện xong sẽ hiển thị như sau Trong giao diện kho chứa ECR, bạn sẽ thấy Docker Image đã được đẩy lên thành công. Bạn đã hoàn tất xây dựng hệ thống gợi ý sản phẩm cho một cửa hàng thời trang online",
    "description": "Chuẩn bị Trong phần này yêu cầu trên máy tính của bạn đã cài đặt Docker và AWS CLI. Bạn có thể tham khảo hướng dẫn cài đặt Docker tại Docker Installation và AWS CLI tại AWS CLI Installation.\nCác bước thực hiện Truy cập vào AWS Management Console và tìm kiếm dịch vụ Amazon ECR bằng cách gõ ECR. Trong giao diện Amazon ECR, chọn Create từ menu bên trái. Tại giao diện tạo kho chứa (repository), điền các thông tin sau: Repository name: fashion-ecr-repository Sau đó bấm Create để tạo kho chứa. Tại giao diện kho chứa vừa tạo, chọn repository vừa tạo và bấm vào View push commands để xem các lệnh cần thiết để đẩy Docker Image lên kho chứa.",
    "tags": [],
    "title": "7.2 Tạo ECR Image cho Lambda Function",
    "uri": "/recommended/2-create-ecr-image/index.html"
  },
  {
    "breadcrumb": "AWS Fashion Pipeline \u003e  2. Các bước chuẩn bị",
    "content": "Tổng quan Ở bước này, chúng ta sẽ tạo một EC2 Instance để chạy giả lập một ứng dụng Website bán hàng. Và trong phần này sẽ tạo EC2 Instance và cài đặt môi trường cho ứng dụng bao gồm:\nMôi trường Python Môi trường PostgreSQL Môi trường Git Và cài đặt các thư viện cần thiết cho ứng dụng Tạo Elastic Compute Cloud (EC2) Instance Truy cập Amazon Management Console Tìm kiếm dịch vụ EC2 Chọn EC2 từ kết quả tìm kiếm Chọn Launch Instance Trong giao diện Launch Instance Name: fashion-webapp Amazon Machine Image (AMI): Amazon Linux 2023 AMI Instance Type: t2.micro Tại phần Key pair, chọn Create new key pair\nKey pair name: fashion-keypair Key pair type: RSA, private key file format: .pem Chọn Create key pair Tại phần Network Settings, chọn Edit:\nVPC: fashion-vpc Subnet: fashion-subnet-public1-ap-southeast-1a Auto-assign Public IP: Enable Security Group chọn Create security group security group name: fashion-webapp-sg description: Allow SSH from My IP, and HTTP, HTTPS from everywhere Tại Inbound Security Group Rules, cài đặt như hình sau Còn lại để mặc định, nhấn Launch instance để hoàn tất việc tạo EC2 Instance Cài đặt môi trường bên trong EC2 Instance Kết nối SSH vào EC2 Instance Di chuyển tới thư mục chứa file fashion-keypair.pem Sử dụng terminal, gõ lệnh sau: ssh -i fashion-keypair.pem ec2-user@\u003cPublic IP | Public DNS\u003e Gõ ‘yes’ Nếu không kết nối được, hãy kiểm tra lại Security Group đã cho phép SSH từ IP của bạn chưa nhé. Cài đặt môi trường Python, Git, PostgreSQL cho EC2 Instance Đầu tiên, gõ sudo su để chuyển sang user root Cài đặt các gói cần thiết (quá trình mất khoảng 5-10 phút): dnf install -y git tar gcc \\ zlib-devel bzip2-devel readline-devel \\ sqlite sqlite-devel openssl-devel \\ tk-devel libffi-devel xz-devel curl https://pyenv.run | bash \u0026\u0026 \\ echo 'export PYENV_ROOT=\"$HOME/.pyenv\"' \u003e\u003e ~/.bashrc \u0026\u0026 \\ echo '[[ -d $PYENV_ROOT/bin ]] \u0026\u0026 export PATH=\"$PYENV_ROOT/bin:$PATH\"' \u003e\u003e ~/.bashrc \u0026\u0026 \\ echo 'eval \"$(pyenv init -)\"' \u003e\u003e ~/.bashrc \u0026\u0026 \\ source ~/.bashrc \u0026\u0026 \\ pyenv install 3.12.4 \u0026\u0026 \\ pyenv global 3.12.4 sudo yum install postgresql17-server.x86_64 -y Kiểm tra lại phiên bản Python đã cài đặt python --version Kiểm tra lại phiên bản PostgreSQL đã cài đặt psql --version Clone repository từ github về EC2 Instance git clone --sparse --filter=blob:none https://github.com/ltdungg/aws-fashion-data-pipeline.git project cd project git sparse-checkout init git sparse-checkout set ec2 cd ec2 Cài đặt các thư viện cần thiết pip install -r requirements.txt Gắn role cho EC2 để ghi vào Kinesis Data Stream Về giao diện EC2 Instances bấm vào fashion-webapp, chọn Actions, Security, Modify IAM role. Chọn ec2-kinesis-role mà chúng ta đã tạo ở phần trước. Bấm Update IAM role.",
    "description": "Tổng quan Ở bước này, chúng ta sẽ tạo một EC2 Instance để chạy giả lập một ứng dụng Website bán hàng. Và trong phần này sẽ tạo EC2 Instance và cài đặt môi trường cho ứng dụng bao gồm:\nMôi trường Python Môi trường PostgreSQL Môi trường Git Và cài đặt các thư viện cần thiết cho ứng dụng Tạo Elastic Compute Cloud (EC2) Instance Truy cập Amazon Management Console Tìm kiếm dịch vụ EC2 Chọn EC2 từ kết quả tìm kiếm",
    "tags": [],
    "title": "2.3 Tạo EC2 Instance",
    "uri": "/preparation/setup-ec2/index.html"
  },
  {
    "breadcrumb": "AWS Fashion Pipeline",
    "content": "Tổng quan Trong phần này, ta sẽ tạo môi trường ứng dụng cho Data Pipeline. Môi trường bao gồm: EC2 Instance: Nơi tạo ra dữ liệu giả và khởi tạo các table database. RDS: Database PostgreSQL để lưu trữ dữ liệu giả. Mô hình database bao gồm 4 bảng:\nproducts, users, orders và order_details Các bước triển khai Quay lại giao diện RDS Chọn Databases từ menu bên trái Bấm vào fashion-db Tại giao diện fashion-db, lưu lại Endpoint của database vào một nơi nào đó. Kéo xuống phần Connected compute resources Bấm vào Actions Chọn Set up EC2 connection Tại giao diện Set up EC2 connection Phần EC2 Instance, chọn fashion-webapp Chọn continue Tại phần Review and confirm, chọn Set up Khi hoàn tất, bạn sẽ thấy một thông báo như sau: Quay trở lại terminal kết nối với EC2 Instance tại bước 2.3 Tạo EC2 Instance.\nTest kết nối tới database PostgreSQL với DNS Database bạn đã lưu lúc trước:\nDùng lệnh sau để kết nối tới database PostgreSQL, sau đó nhập password database bạn vừa tạo. psql -U postgres -h \u003cYOUR_POSTGRESQL_DNS\u003e -p 5432 -d postgres Nếu bạn thấy thông báo như sau thì bạn đã kết nối thành công tới database PostgreSQL.\nCòn nếu không hiện lên đúng hình ảnh phía dưới, xem lại các bước trên nhé. Gõ \\q để thoát khỏi database PostgreSQL. và trở về terminal.\nTạo môi trường cho việc tạo dữ liệu giả:\nGõ lệnh sau: vim .env Tại vim, bấm i để vào chế độ nhập liệu. Nhập các thông tin sau vào file .env: RDS_HOST: là DNS của database PostgreSQL bạn đã lưu ở bước 2. RDS_PASSWORD: là password của database PostgreSQL bạn đã tạo ở bước 2. KINESIS_STREAM_NAME: fashion-ds STREAM_ARN: Là ARN của Kinesis Stream bạn đã tạo ở bước 2.6 Tạo Kinesis Stream. Bấm Esc để thoát khỏi chế độ nhập liệu. Gõ :wq để lưu lại file .env và thoát khỏi vim. Tạo table cho database : Trong phần này, ta sẽ tạo 4 bảng cho database trên RDS Đồng thời cũng tạo ra dữ liệu giả cho 1000 users và bảng products python initdb.py Nếu bạn thấy thông báo như sau thì bạn đã tạo thành công database và dữ liệu giả cho 1000 users và bảng products. Xem các bảng đã tạo trong database PostgreSQL:\nGõ lệnh tại bước 7 để kết nối với database PostgreSQL. Gõ lệnh sau để xem các bảng đã tạo trong database PostgreSQL: SELECT * FROM information_schema.tables WHERE table_schema = 'public'; Xem tất cả dữ liệu trong bảng (Tùy chọn): SELECT * FROM \u003cTABLE_NAME\u003e; --- Ví dụ: SELECT * FROM users; Bạn đã thành công trong việc tạo môi trường ứng dụng cho Data Pipeline.",
    "description": "Tổng quan Trong phần này, ta sẽ tạo môi trường ứng dụng cho Data Pipeline. Môi trường bao gồm: EC2 Instance: Nơi tạo ra dữ liệu giả và khởi tạo các table database. RDS: Database PostgreSQL để lưu trữ dữ liệu giả. Mô hình database bao gồm 4 bảng:\nproducts, users, orders và order_details Các bước triển khai Quay lại giao diện RDS Chọn Databases từ menu bên trái Bấm vào fashion-db",
    "tags": [],
    "title": "3. Tạo môi trường ứng dụng",
    "uri": "/generate-data/index.html"
  },
  {
    "breadcrumb": "AWS Fashion Pipeline \u003e  5. Test dữ liệu",
    "content": "Chạy thử Lambda Function Vào trang của Lambda Function lambda-rds-to-s3 trong AWS Console của bạn. Bấm vào Test Vì mục đích của Lambda theo yêu cầu của sếp là sau một ngày thì sẽ chạy một lần, nên đoạn code sẽ chạy vào đầu ngày hôm sau tức là lúc 00:01 ngày hôm sau. Nên muốn test thì phải thêm Event JSON và DATE_EXECUTION là ngày bạn chạy lab theo định dạng Năm-Tháng-Ngày. Ví dụ “2025-04-17” Trong phần Test: Chọn Create new event Nhập tên cho event là test Event JSON như sau: { \"DATE_EXECUTION\": \"\u003cNgày chạy workshop\u003e\" } Bấm Test\nBạn đã thành công chạy test\nVào lại S3 bucket fashion-landing-zone và kiểm tra xem đã có dữ liệu chưa nhé.",
    "description": "Chạy thử Lambda Function Vào trang của Lambda Function lambda-rds-to-s3 trong AWS Console của bạn. Bấm vào Test Vì mục đích của Lambda theo yêu cầu của sếp là sau một ngày thì sẽ chạy một lần, nên đoạn code sẽ chạy vào đầu ngày hôm sau tức là lúc 00:01 ngày hôm sau. Nên muốn test thì phải thêm Event JSON và DATE_EXECUTION là ngày bạn chạy lab theo định dạng Năm-Tháng-Ngày. Ví dụ “2025-04-17” Trong phần Test: Chọn Create new event Nhập tên cho event là test Event JSON như sau: { \"DATE_EXECUTION\": \"\u003cNgày chạy workshop\u003e\" }",
    "tags": [],
    "title": "5.3 Test Lambda Function",
    "uri": "/test-connection/test-lambda/index.html"
  },
  {
    "breadcrumb": "AWS Fashion Pipeline \u003e  7. Tạo hệ thống gợi ý sản phẩm",
    "content": "Các bước thực hiện Truy cập vào AWS Management Console và tìm kiếm dịch vụ Lambda bằng cách gõ Lambda. Trong giao diện Lambda, chọn Create function.\nTại giao diện Create function, chọn Container image.\nFunction name: kinesis-to-dynamodb Container image URI: Chọn Browse images và chọn kho chứa ECR fashion-ecr-repository và Docker Image đã tạo ở bước trước. Nhớ chọn image vơới tag là lastest. Execution role: Chọn Use an existing role và chọn role lambda-kinesis-dynamodb-role đã tạo ở bước trước. Chọn Create function để tạo Lambda Function. Sau khi tạo xong, vào Configuration của Lambda Function, trong phần VPC chọn Edit và cấu hình như sau:\nChọn General configuration, chọn Edit và cấu hình timeout là 5 phút và Memory là 512MB. Xuống phần Environment chọn Edit và thêm các biến môi trường sau: KINESIS_DATA_STREAM_NAME: fashion-ds KINESIS_DATA_STREAM_ARN: Lấy ARN của Kinesis Data Stream fashion-ds từ AWS Kinesis Data Streams. DYNAMODB_TABLE_NAME: fashion-rcm-table CLEAN_ZONE_DATABASE_CATALOG: fashion-clean-zone Tạo Trigger cho Lambda Function:\nMục đích: Kết nối Lambda Function với Kinesis Data Stream để nhận dữ liệu từ stream mỗi khi có sự kiện mới.\nTrong giao diện chính của Lambda Function, chọn Add trigger. Chọn Kinesis từ danh sách dịch vụ. Chọn Kinesis Data Stream fashion-ds đã tạo ở bước trước.\nChọn Batch size là 1 (số lượng bản ghi tối đa mà Lambda sẽ xử lý trong một lần gọi).\nChọn Add",
    "description": "Các bước thực hiện Truy cập vào AWS Management Console và tìm kiếm dịch vụ Lambda bằng cách gõ Lambda. Trong giao diện Lambda, chọn Create function.\nTại giao diện Create function, chọn Container image.\nFunction name: kinesis-to-dynamodb Container image URI: Chọn Browse images và chọn kho chứa ECR fashion-ecr-repository và Docker Image đã tạo ở bước trước. Nhớ chọn image vơới tag là lastest. Execution role: Chọn Use an existing role và chọn role lambda-kinesis-dynamodb-role đã tạo ở bước trước. Chọn Create function để tạo Lambda Function.",
    "tags": [],
    "title": "7.3 Tạo Lambda Function",
    "uri": "/recommended/3-create-lambda-func/index.html"
  },
  {
    "breadcrumb": "AWS Fashion Pipeline \u003e  2. Các bước chuẩn bị",
    "content": "Cài đặt Relational Database Service (RDS) Truy cập Amazon Management Console Tìm kiếm dịch vụ RDS Chọn Aurora and RDS từ kết quả tìm kiếm Chọn Create database Tại giao diện tạo RDS, chọn Standard Create Engine options: PostgreSQL Version: PostgreSQL 17.2-R2 Templates: Free Tier DB instance identifier: fashion-db Master password: Tự đặt password cho riêng mình DB instance class: db.t4g.micro Tại phần Connectivity, chọn Edit VPC: fashion-vpc Subnet group: fashion-subnet-group Public access: No VPC security group: Chọn Create new Security group name: fashion-db-sg Availability zone: ap-southeast-1a Còn lại để mặc định và nhấn Create database để hoàn tất việc tạo RDS",
    "description": "Cài đặt Relational Database Service (RDS) Truy cập Amazon Management Console Tìm kiếm dịch vụ RDS Chọn Aurora and RDS từ kết quả tìm kiếm Chọn Create database Tại giao diện tạo RDS, chọn Standard Create Engine options: PostgreSQL Version: PostgreSQL 17.2-R2 Templates: Free Tier DB instance identifier: fashion-db Master password: Tự đặt password cho riêng mình DB instance class: db.t4g.micro Tại phần Connectivity, chọn Edit VPC: fashion-vpc Subnet group: fashion-subnet-group Public access: No VPC security group: Chọn Create new Security group name: fashion-db-sg Availability zone: ap-southeast-1a",
    "tags": [],
    "title": "2.4 Tạo RDS",
    "uri": "/preparation/setup-rds/index.html"
  },
  {
    "breadcrumb": "AWS Fashion Pipeline",
    "content": "Tổng quan Trong phần này, chúng ta sẽ sử dụng AWS Lambda để trích xuất dữ liệu từ Amazon RDS và lưu vào Amazon S3. Chúng ta sẽ sử dụng một hàm Lambda đơn giản để thực hiện việc này. Hàm Lambda sẽ được kích hoạt sẽ kết nối đến Amazon RDS, thực hiện truy vấn SQL để lấy dữ liệu và sau đó lưu dữ liệu vào Amazon S3 dưới dạng file CSV. Đầu tiên tải zip file Lambda function tại đây, bấm nút download raw file góc bên phải màn hình để tải xuống. Truy cập vào S3 Truy cập vào AWS Console, tìm kiếm và chọn S3 từ menu dịch vụ. Bấm vào bucket fashion-logic-zone mà chúng ta đã tạo ở phần trước. Tại giao diện bucket, bấm Upload 4. Tại giao diện Upload:\nBấm Add files, chọn file rds-to-s3-lambda.zip mà chúng ta đã tải xuống ở trên. Bấm Upload. Sau khi upload thành công, chúng ta sẽ thấy file rds-to-s3-lambda.zip trong bucket. Lưu lại Object URL của file này, chúng ta sẽ sử dụng nó trong hàm Lambda. Tạo Lambda function Truy cập vào AWS Console, tìm kiếm và chọn Lambda từ menu dịch vụ. Bấm vào Create function.\nTại giao diện Create function Chọn Author from scratch. Nhập tên cho function là lambda-rds-to-s3. Chọn Python 3.13 cho Runtime. Bấm Change default execution role. Chọn Use an existing role. Chọn role lambda-s3-full-access mà chúng ta đã tạo ở phần trước. Bấm Create function. Tại giao diện Function code: Tại phần Code source, bấm vào Upload from và chọn Amazon S3 location. Nhập đường dẫn đến file rds-to-s3-lambda.zip mà chúng ta đã upload lên S3 ở phần trước. Bấm Save. Sau khi upload thành công, bấm vào Configuration Ở phần General configuration, bấm Edit. Tại phần Memory tăng giá trị lên 512 MB. Tại phần Timeout, nhập giá trị là 5 minutes. Bấm Save. Tại giao diện Configuration, bấm vào Environment variables. Bấm Edit. Bấm Add environment variable. Nhập các biến môi trường sau: RDS_HOST: Địa chỉ endpoint của Amazon RDS mà chúng ta đã tạo ở phần trước. RDS_PASSWORD: Mật khẩu của user postgres mà chúng ta đã tạo ở phần trước. S3_BUCKET: fashion-landing-zone. Bấm Save. Tại giao diện Configuration, bấm vào VPC. Chọn Edit Tại phần VPC, chọn VPC mà chúng ta đã tạo ở phần trước. Tại phần Subnets, chọn 2 subnet private mà chúng ta đã tạo ở phần trước. fashion-subnet-private1-ap-southeast-1a fashion-subnet-private2-ap-southeast-1b Select Security groups chọn default security group của VPC. Bấm Save. Cho phép Lambda truy cập vào RDS Quay lại giao diện fashion-db tại phần Databases tại giao diện RDS. Kéo xuống phần Connected compute resources, bấm vào Acion, chọn Set up Lambda connection Trong giao diện Set up Lambda connection Chọn choosing existing function Chọn hàm lambda-rds-to-s3 mà chúng ta đã tạo ở phần trước. Tắt tùy chọn Connect using Proxy Bấm Set up",
    "description": "Tổng quan Trong phần này, chúng ta sẽ sử dụng AWS Lambda để trích xuất dữ liệu từ Amazon RDS và lưu vào Amazon S3. Chúng ta sẽ sử dụng một hàm Lambda đơn giản để thực hiện việc này. Hàm Lambda sẽ được kích hoạt sẽ kết nối đến Amazon RDS, thực hiện truy vấn SQL để lấy dữ liệu và sau đó lưu dữ liệu vào Amazon S3 dưới dạng file CSV. Đầu tiên tải zip file Lambda function tại đây, bấm nút download raw file góc bên phải màn hình để tải xuống. Truy cập vào S3 Truy cập vào AWS Console, tìm kiếm và chọn S3 từ menu dịch vụ. Bấm vào bucket fashion-logic-zone mà chúng ta đã tạo ở phần trước. Tại giao diện bucket, bấm Upload 4. Tại giao diện Upload:",
    "tags": [],
    "title": "4. Trích xuất dữ liệu từ RDS vào S3",
    "uri": "/rds-to-s3/index.html"
  },
  {
    "breadcrumb": "AWS Fashion Pipeline \u003e  7. Tạo hệ thống gợi ý sản phẩm",
    "content": "Chạy thử hệ thống gợi ý sản phẩm Kết nối với EC2 instance fashion-webapp đã tạo ở bước trước. Và chạy lệnh sau: python data-generator.py Lưu ý: Đảm bảo người dùng là root user bằng cách dùng sudo su trước khi chạy lệnh trên.\nChọn dịch vụ CloudWatch chọn Log groups chọn /aws/lambda/kinesis-to-dynamodb để xem log của Lambda Function kinesis-to-dynamodb. Chọn dịch vụ DynamoDB bấm chọn Tables bên góc trái màn hình và bấm vào fashion-rcm-table Trong giao diện của bảng, chọn Explore table items để xem các gợi ý sản phẩm đã được lưu trữ.",
    "description": "Chạy thử hệ thống gợi ý sản phẩm Kết nối với EC2 instance fashion-webapp đã tạo ở bước trước. Và chạy lệnh sau: python data-generator.py Lưu ý: Đảm bảo người dùng là root user bằng cách dùng sudo su trước khi chạy lệnh trên.\nChọn dịch vụ CloudWatch chọn Log groups chọn /aws/lambda/kinesis-to-dynamodb để xem log của Lambda Function kinesis-to-dynamodb. Chọn dịch vụ DynamoDB bấm chọn Tables bên góc trái màn hình và bấm vào fashion-rcm-table Trong giao diện của bảng, chọn Explore table items để xem các gợi ý sản phẩm đã được lưu trữ.",
    "tags": [],
    "title": "7.4 Kiểm tra hệ thống gợi ý sản phẩm",
    "uri": "/recommended/4-test/index.html"
  },
  {
    "breadcrumb": "AWS Fashion Pipeline \u003e  2. Các bước chuẩn bị",
    "content": "Giới thiệu Trong phần này, ta sẽ tạo S3 Bucket để làm một Data Lake cho Data Pipeline. Trong phần này ta sẽ tạo các bucket sau:\nfashion-landing-zone: Đây là nơi lưu trữ dữ liệu thô chưa được xử lý. fashion-clean-zone: Đây là nơi lưu trữ dữ liệu đã được chuyển đổi và chuẩn bị sẵn sàng cho các công việc phân tích. fashion-logic-zone: Đây là nơi lưu trữ những Lambda Function và các dags chạy Airflow cho AWS MWAA. Cài đặt Truy cập Amazon Management Console Tìm kiếm dịch vụ S3 Chọn S3 từ kết quả tìm kiếm Sau đó bấm Create bucket Trong giao diện Create bucket Chọn Bucket name là fashion-landing-zone Để mặc định các thông số còn lại Bấm Create bucket Lặp lại bước 2 với các bucket sau: fashion-clean-zone fashion-logic-zone Bạn đang tạo thành công 3 bucket S3.",
    "description": "Giới thiệu Trong phần này, ta sẽ tạo S3 Bucket để làm một Data Lake cho Data Pipeline. Trong phần này ta sẽ tạo các bucket sau:\nfashion-landing-zone: Đây là nơi lưu trữ dữ liệu thô chưa được xử lý. fashion-clean-zone: Đây là nơi lưu trữ dữ liệu đã được chuyển đổi và chuẩn bị sẵn sàng cho các công việc phân tích. fashion-logic-zone: Đây là nơi lưu trữ những Lambda Function và các dags chạy Airflow cho AWS MWAA. Cài đặt Truy cập Amazon Management Console Tìm kiếm dịch vụ S3 Chọn S3 từ kết quả tìm kiếm Sau đó bấm Create bucket",
    "tags": [],
    "title": "2.5 Tạo S3 Bucket",
    "uri": "/preparation/setup-s3/index.html"
  },
  {
    "breadcrumb": "AWS Fashion Pipeline",
    "content": "Tạo dữ liệu để test. Vào lại terminal kết nối với EC2 Instances. Chạy lệnh sau đi đến thư mục chứa file tạo dữ liệu: cd /home/ec2-user/project/ec2 Chạy lệnh sau để tạo dữ liệu: python data-generator.py Sau khi chạy một lúc (để khoảng 5 phút), hãy bấm Ctrl + C để dừng lại. Ta có thể thấy dữ liệu output trên terminal như sau. Bạn có thể thấy như sau: Order data insert successfully là dữ liệu order được đẩy vào RDS. Còn lại là dữ liệu tương tác khách hàng được đẩy vào Kinesis stream, với những event như page_view, add_to_cart, product_view.",
    "description": "Tạo dữ liệu để test. Vào lại terminal kết nối với EC2 Instances. Chạy lệnh sau đi đến thư mục chứa file tạo dữ liệu: cd /home/ec2-user/project/ec2 Chạy lệnh sau để tạo dữ liệu: python data-generator.py Sau khi chạy một lúc (để khoảng 5 phút), hãy bấm Ctrl + C để dừng lại. Ta có thể thấy dữ liệu output trên terminal như sau. Bạn có thể thấy như sau: Order data insert successfully là dữ liệu order được đẩy vào RDS. Còn lại là dữ liệu tương tác khách hàng được đẩy vào Kinesis stream, với những event như page_view, add_to_cart, product_view.",
    "tags": [],
    "title": "5. Test dữ liệu",
    "uri": "/test-connection/index.html"
  },
  {
    "breadcrumb": "AWS Fashion Pipeline \u003e  2. Các bước chuẩn bị",
    "content": "Tạo Kinesis Data Stream Truy cập Amazon Management Console Tìm kiếm dịch vụ Kinesis Chọn Kinesis từ kết quả tìm kiếm Trong phần giao diện Kinesis chọn Kinesis Data Streams rồi chọn Create data stream Trong giao diện Create data stream Chọn Name là fashion-ds Chọn Provisioned Phần Provisioned shards nhập 1 Chọn Create data stream Sau khi tạo xong bạn sẽ thấy giao diện như sau, lưu lại ARN của Kinesis Data Stream này để sử dụng trong các bước sau.",
    "description": "Tạo Kinesis Data Stream Truy cập Amazon Management Console Tìm kiếm dịch vụ Kinesis Chọn Kinesis từ kết quả tìm kiếm Trong phần giao diện Kinesis chọn Kinesis Data Streams rồi chọn Create data stream Trong giao diện Create data stream Chọn Name là fashion-ds Chọn Provisioned Phần Provisioned shards nhập 1 Chọn Create data stream Sau khi tạo xong bạn sẽ thấy giao diện như sau, lưu lại ARN của Kinesis Data Stream này để sử dụng trong các bước sau.",
    "tags": [],
    "title": "2.6 Tạo Kinesis Data Stream",
    "uri": "/preparation/setup-kinesis/index.html"
  },
  {
    "breadcrumb": "AWS Fashion Pipeline",
    "content": "Các bước thực hiện Trong phần này chúng ta sẽ sử dụng Glue ETL để thực hiện chuyển đổi dữ liệu từ S3 bucket fashion-landing-zone sang S3 bucket fashion-clean-zone. Với mục đích sẵn sàng cho việc phân tích dữ liệu cũng như lưu trữ dữ liệu.\nPhần này sẽ bao gồm các bước sau:\nTạo Glue Job để chuyển đổi dữ liệu từ S3 bucket fashion-landing-zone sang S3 bucket fashion-clean-zone Tạo Glue Trigger để tự động chạy Glue Job Kiểm tra dữ liệu trong S3 bucket fashion-clean-zone Tạo Glue Crawler để tự động tạo Glue Data Catalog cho dữ liệu trong S3 bucket fashion-clean-zone",
    "description": "Các bước thực hiện Trong phần này chúng ta sẽ sử dụng Glue ETL để thực hiện chuyển đổi dữ liệu từ S3 bucket fashion-landing-zone sang S3 bucket fashion-clean-zone. Với mục đích sẵn sàng cho việc phân tích dữ liệu cũng như lưu trữ dữ liệu.\nPhần này sẽ bao gồm các bước sau:\nTạo Glue Job để chuyển đổi dữ liệu từ S3 bucket fashion-landing-zone sang S3 bucket fashion-clean-zone Tạo Glue Trigger để tự động chạy Glue Job Kiểm tra dữ liệu trong S3 bucket fashion-clean-zone Tạo Glue Crawler để tự động tạo Glue Data Catalog cho dữ liệu trong S3 bucket fashion-clean-zone",
    "tags": [],
    "title": "6. Chuyển đổi dữ liệu",
    "uri": "/transform/index.html"
  },
  {
    "breadcrumb": "AWS Fashion Pipeline \u003e  2. Các bước chuẩn bị",
    "content": "Tạo Kinesis Firehose Truy cập Amazon Management Console Tìm kiếm dịch vụ Firehose Chọn Amazon Data Firehose từ kết quả tìm kiếm Trong phần giao diện Kinesis Firehose chọn Create Firehose stream Trong giao diện Create Firehose stream Chọn Source là Amazon Kinesis Data Stream Chọn Destination là Amazon S3 Chọn Name là fashion-ds-firehose Trong source settings chọn browse và chọn Kinesis Data Stream đã tạo ở bước trước. Trong phần Destination setting chọn Browse và chọn bucket fashion-landing-zone. Phần New line delimiter: Chọn Enabled Tại S3 bucket prefix nhập clickstreams/ Dưới buffer hints, compression, file extension and encryption: Để mặc định bufer size là 5MB Để mặc định bufer interval là 300s Chọn Create Firehose stream",
    "description": "Tạo Kinesis Firehose Truy cập Amazon Management Console Tìm kiếm dịch vụ Firehose Chọn Amazon Data Firehose từ kết quả tìm kiếm Trong phần giao diện Kinesis Firehose chọn Create Firehose stream Trong giao diện Create Firehose stream Chọn Source là Amazon Kinesis Data Stream Chọn Destination là Amazon S3 Chọn Name là fashion-ds-firehose Trong source settings chọn browse và chọn Kinesis Data Stream đã tạo ở bước trước. Trong phần Destination setting chọn Browse và chọn bucket fashion-landing-zone. Phần New line delimiter: Chọn Enabled Tại S3 bucket prefix nhập clickstreams/ Dưới buffer hints, compression, file extension and encryption: Để mặc định bufer size là 5MB Để mặc định bufer interval là 300s Chọn Create Firehose stream",
    "tags": [],
    "title": "2.7 Tạo Kinesis Firehose",
    "uri": "/preparation/setup-firehose/index.html"
  },
  {
    "breadcrumb": "AWS Fashion Pipeline",
    "content": "Giới thiệu Trong phần này, chúng ta sẽ xây dựng một hệ thống gợi ý sản phẩm dựa trên dữ liệu đã được chuyển đổi và lưu trữ trong S3 bucket fashion-clean-zone. Hệ thống này sẽ sử dụng Amazon DynamoDB để lưu trữ các gợi ý sản phẩm cho người dùng.\nHệ thống này hoạt động dựa trên phân tích dữ liệu người dùng theo gần thời gian thực, bằng cách mỗi khi nguười dùng thao tác với sản phẩm như xem, thêm vaào giỏ hàng thì hệ thống sẽ ghi nhận và đưa ra những gợi ý cho các sản phẩm liên quan\nCác bước thực hiện Tạo DynamoDB Table để hiện gợi ý sản phẩm cho người dùng Tạo Amazon ECR để lưu trữ Docker Image cho Lambda Function Tạo Lambda Function để xử lý các sự kiện từ S3 bucket fashion-clean-zone và cập nhật gợi ý sản phẩm vào DynamoDB Kiểm tra hệ thống gợi ý sản phẩm hoạt động",
    "description": "Giới thiệu Trong phần này, chúng ta sẽ xây dựng một hệ thống gợi ý sản phẩm dựa trên dữ liệu đã được chuyển đổi và lưu trữ trong S3 bucket fashion-clean-zone. Hệ thống này sẽ sử dụng Amazon DynamoDB để lưu trữ các gợi ý sản phẩm cho người dùng.\nHệ thống này hoạt động dựa trên phân tích dữ liệu người dùng theo gần thời gian thực, bằng cách mỗi khi nguười dùng thao tác với sản phẩm như xem, thêm vaào giỏ hàng thì hệ thống sẽ ghi nhận và đưa ra những gợi ý cho các sản phẩm liên quan",
    "tags": [],
    "title": "7. Tạo hệ thống gợi ý sản phẩm",
    "uri": "/recommended/index.html"
  },
  {
    "breadcrumb": "AWS Fashion Pipeline",
    "content": "Giới thiệu Trong phần này, chúng ta sẽ thiết lập các nhiệm vụ định kỳ để tự động hóa quá trình cập nhật dữ liệu và gợi ý sản phẩm trong hệ thống. Chúng ta sẽ sử dụng Amazon EventBridge để lên lịch cho các nhiệm vụ này.\nCác bước thực hiện Trong AWS Management Console, tìm kiếm và chọn dịch vụ EventBridge. Chọn EventBridge Schedule rồi chọn Create rules ở thanh bên trái. Tại giao diện Create rule, điền các thông tin sau: Name: fashion-rds-to-s3-scheduler Description: Rule to trigger Lambda function for ETL process Dưới phần Schedule Pattern chọn Recurring schedule Timezone: (UTC +07:00) Asia/Saigon Cron expression: cron(0 0 * * ? *) (điều này có nghĩa là nhiệm vụ sẽ chạy mỗi ngày lúc 00:00 giờ Việt Nam) Flexible time window: Để 5 phút (5 minutes) Chọn Next Trong phần Select targets, chọn AWS Lambda và chọn Lambda Function fashion-rds-to-s3 đã tạo ở bước trước. Bấm Next để xem lại cấu hình và sau đó bấm Create schedule để hoàn tất việc tạo rule.",
    "description": "Giới thiệu Trong phần này, chúng ta sẽ thiết lập các nhiệm vụ định kỳ để tự động hóa quá trình cập nhật dữ liệu và gợi ý sản phẩm trong hệ thống. Chúng ta sẽ sử dụng Amazon EventBridge để lên lịch cho các nhiệm vụ này.\nCác bước thực hiện Trong AWS Management Console, tìm kiếm và chọn dịch vụ EventBridge. Chọn EventBridge Schedule rồi chọn Create rules ở thanh bên trái.",
    "tags": [],
    "title": "8. Lên lịch cho các nhiệm vụ định kỳ",
    "uri": "/event-bridge/index.html"
  },
  {
    "breadcrumb": "AWS Fashion Pipeline",
    "content": "S3 Vào dịch vụ S3, chọn các bucket đã tạo trong quá trình thực hiện dự án. Chọn Empty để xóa tất cả các đối tượng trong bucket. Sau khi xóa, chọn Delete để xóa bucket. DynamoDB Vào dịch vụ DynamoDB, chọn Tables từ menu bên trái. Chọn bảng fashion-rcm-table đã tạo trong quá trình thực hiện dự án. Chọn Delete để xóa bảng. Xác nhận việc xóa bảng bằng cách nhập confirm và bấm Delete. Glue Vào dịch vụ Glue, chọn ETL jobs từ menu bên trái.\nChọn job fashion-datalake-etl rồi bấm vào Actions chọn Delete jobs.\nChọn Databases từ thanh menu bên trái.\nChọn database fashion-clean-zone bấm Delete.\nChọn Crawlers từ menu bên trái.\nChọn crawler đã tạo, bấm Actions và chọn Delete crawler. Gõ Delete để xác nhận và bấm Delete.\nRDS Vào dịch vụ RDS, chọn Databases từ menu bên trái. Chọn database fashion-db đã tạo trong quá trình thực hiện dự án. Bấm vào Actions và chọn Delete. Lambda Vào dịch vụ Lambda, chọn Functions từ menu bên trái. Chọn các Lambda function đã tạo fashion-rds-to-s3, kinesis-to-dynamodb. Bấm vào Actions và chọn Delete. Xác nhận việc xóa bằng cách nhập confirm và bấm Delete. EventBridge Vào dịch vụ EventBridge, chọn Schedules từ menu bên trái. Chon rule đã tạo fashion-rds-to-s3-scheduler. Bấm Delete. EC2 Vào dịch vụ EC2, chọn Instances từ menu bên trái. Chọn instance fashion-webapp đã tạo. Bấm vào Instance state và chọn Terminate instance. Bấm Terminate (delete) để xác nhận việc xóa instance. ECR Vào dịch vụ ECR, chọn Repositories từ menu bên trái. Chọn repository fashion-ecr-repository đã tạo. Bấm Delete IAM Vào dịch vụ IAM, chọn Roles từ menu bên trái. Chọn các role đã tạo: AWS-Glue-S3-Full-Access AWSGlueServiceRole-FashionCrawlerRole ec2-kinesis-role lambda-s3-full-access lambda-kinesis-dynamodb-role Xóa các role Kinesis Firehose và EventBridge nếu có. Bấm Delete gõ delete để xác nhận và bấm Delete. Network Interfaces Các Network Interfaces được tạo ra bởi Lambda Function sẽ được tự động xóa khi Lambda Function bị xóa khoảng 20-40 phút. Bạn sẽ không thể xóa VPC nếu vẫn còn Network Interfaces đang hoạt động. Do đó, bạn cần đợi cho đến khi các Network Interfaces này được xóa. Nếu bạn muốn xóa Network Interfaces ngay lập tức, hãy xem trên https://repost.aws/knowledge-center/lambda-eni-find-delete VPC Sau khi các Network Interfaces đã được xóa, bạn có thể xóa VPC fashion-vpc đã tạo trong quá trình thực hiện dự án. Vào dịch vụ VPC, chọn Your VPCs từ menu bên trái. Chọn VPC fashion-vpc đã tạo. Bấm vào Actions và chọn Delete VPC. Xác nhận việc xóa bằng cách nhập delete và bấm Delete.",
    "description": "S3 Vào dịch vụ S3, chọn các bucket đã tạo trong quá trình thực hiện dự án. Chọn Empty để xóa tất cả các đối tượng trong bucket. Sau khi xóa, chọn Delete để xóa bucket. DynamoDB Vào dịch vụ DynamoDB, chọn Tables từ menu bên trái. Chọn bảng fashion-rcm-table đã tạo trong quá trình thực hiện dự án. Chọn Delete để xóa bảng. Xác nhận việc xóa bảng bằng cách nhập confirm và bấm Delete. Glue Vào dịch vụ Glue, chọn ETL jobs từ menu bên trái.",
    "tags": [],
    "title": "9. Dọn dẹp các tài nguyên",
    "uri": "/clean/index.html"
  },
  {
    "breadcrumb": "AWS Fashion Pipeline",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Categories",
    "uri": "/categories/index.html"
  },
  {
    "breadcrumb": "AWS Fashion Pipeline",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tags",
    "uri": "/tags/index.html"
  }
]
